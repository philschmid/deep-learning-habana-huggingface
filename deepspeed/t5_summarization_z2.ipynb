{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use DeepSpeed with Habana Gaudi for large-scale efficitent training\n",
    "\n",
    "In this blog, you will learn how to fine-tune [T5-3B](https://huggingface.co/t5-3b) for abstractive summarization with [DeepSpeed](https://www.deepspeed.ai/) using a Habana Gaudi-based [DL1 instance](https://aws.amazon.com/ec2/instance-types/dl1/) on AWS to take advantage of the cost performance benefits of Gaudi. We will use the Hugging Faces Transformers, Optimum Habana and Datasets library as well as the Habana fork of [DeepSpeed](https://github.com/HabanaAI/DeepSpeed). We are going to fine-tune fine-tune [T5-3B](https://huggingface.co/t5-3b) using the [Trade the Event](https://paperswithcode.com/paper/trade-the-event-corporate-events-detection) dataset for abstractive text summarization. The benchmark dataset contains 303893 news articles range from 2020/03/01 to 2021/05/06. The articles are downloaded from the [PRNewswire](https://www.prnewswire.com/) and [Businesswire](https://www.businesswire.com/).\n",
    "\n",
    "\n",
    "You will learn how to:\n",
    "\n",
    "1. [Prepare Dataset & Environment](#1-prepare-dataset--environment)\n",
    "2. [Configure DeepSpeed](#2-configure-deepspeed)\n",
    "3. [Run T3-3B on Habana Gaudi](#3-run-t3-3b-on-habana-gaudi)\n",
    "4. [Cost performance benefits of Habana Gaudi on AWS](#4-cost-performance-benefits-of-habana-gaudi-on-aws)\n",
    "\n",
    "![remote-runner](../assets/remote-runner.png)\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "Before we can start, make sure you have met the following requirements\n",
    "\n",
    "* AWS Account with quota for [DL1 instance type](https://aws.amazon.com/ec2/instance-types/dl1/)\n",
    "* [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) installed\n",
    "* AWS IAM user [configured in CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) with permission to create and manage ec2 instances\n",
    "\n",
    "**Helpful Resources**\n",
    "\n",
    "* [Setup Deep Learning environment for Hugging Face Transformers with Habana Gaudi on AWS](https://www.philschmid.de/getting-started-habana-gaudi)\n",
    "* [Deep Learning setup made easy with EC2 Remote Runner and Habana Gaudi](https://www.philschmid.de/habana-gaudi-ec2-runner)\n",
    "* [Optimum Habana Documentation](https://huggingface.co/docs/optimum/main/en/habana_index)\n",
    "* [Pre-Training BERT with Hugging Face Transformers and Habana Gaudi](https://www.philschmid.de/pre-training-bert-habana)\n",
    "* [Habana DeepSpeed User Guide](https://docs.habana.ai/en/latest/PyTorch/DeepSpeed/DeepSpeed_User_Guide.html)\n",
    "\n",
    "\n",
    "## What is DeepSpeed? \n",
    "\n",
    "[DeepSpeed](https://www.deepspeed.ai/training/) is an easy-to-use deep learning optimization software suite that enables unprecedented scale and speed for Deep Learning Training. [DeepSpeed](https://www.deepspeed.ai/) enables you to fit and train larger models on HPUs thanks to various optimizations. In particular, you can use the two following ZeRO configurations that have been validated to be fully functioning with Gaudi:\n",
    "\n",
    "* ZeRO-1, which partitions the optimizer states across processes.\n",
    "* ZeRO-2, which additionnally partitions the gradients across processes so that each process retains only the gradients corresponding to its portion of the optimizer states.\n",
    "\n",
    "These configurations are fully compatible with Habana Mixed Precision and can thus be used to train your model in bf16 precision.\n",
    "\n",
    "You can find more information about DeepSpeed Gaudi integration [here](https://huggingface.co/docs/optimum/habana_deepspeed).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare Dataset & Environment\n",
    "\n",
    "In this example are we going to use Habana Gaudi on AWS using the DL1 instance for running the training. We will use the [Remote Runner](https://github.com/philschmid/deep-learning-remote-runner) toolkit to easily launch our training on a remote DL1 Instance from our local setup. You can check-out [Deep Learning setup made easy with EC2 Remote Runner and Habana Gaudi](https://www.philschmid.de/habana-gaudi-ec2-runner) if you want to know more about how this works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rm-runner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the use of DeepSpeed on Habana Gaudi as easy as possible have we created a docker image that contains all the necessary dependencies. You can find the docker image on the docker up at [huggingface/optimum-habana:4.23.1-pt1.12.0-synapse1.6.0-deepspeed](https://hub.docker.com/r/huggingface/optimum-habana/tags). The docker image is based on the [Optimum Habana DeepSpeed User Guide](https://huggingface.co/docs/optimum/habana_deepspeed) and contains the following dependencies:\n",
    "\n",
    "* `transformers==4.23.1`\n",
    "* `datasets==2.11.0`\n",
    "* `optimum==1.4.0`\n",
    "* `optimum-habana==1.2.3`\n",
    "* `HabanaAI/DeepSpeed==1.6.1`\n",
    "* `synapseAI==1.6.0`\n",
    "* `torch==1.12.0`\n",
    "* `tensorboard`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# container image used for running the training\n",
    "image_id=\"huggingface/optimum-habana:4.23.1-pt1.12.0-synapse1.6.0-deepspeed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example will use the [Hugging Face Hub](https://huggingface.co/models) as a remote model versioning service. To be able to push our model to the Hub, you need to register on the [Hugging Face](https://huggingface.co/join). \n",
    "If you already have an account you can skip this step. \n",
    "After you have an account, we will use the `notebook_login` util from the `huggingface_hub` package to log into our account and store our token (access key) on the disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure DeepSpeed\n",
    "\n",
    "The [GaudiTrainer](https://huggingface.co/docs/optimum/main/en/habana_trainer) allows us to use DeepSpeed as easily as the [Transformers Trainer](https://huggingface.co/docs/transformers/main_classes/trainer). It will take care of adding all of the DeepSpeed specific methods and checks. To add DeepSpeed to our training we have to: \n",
    "\n",
    "1. Create a DeepSpeed configuration\n",
    "2. Specify the DeepSpeed configuration in the `GaudiTrainer`\n",
    "3. Use `deepspeed` to launch our training\n",
    "   \n",
    "These steps are detailed below. A comprehensive guide about how to use DeepSpeed with the Transformers Trainer is also available [here](https://huggingface.co/docs/transformers/main_classes/deepspeed).\n",
    "\n",
    "\n",
    "### 1. Create DeepSpeed configuration\n",
    "\n",
    "The DeepSpeed configuration is passed through as JSON file and enables you to choose the optimizations to apply. Below you will find the confiugration we are going to use for fine-tuning `T5-3b` using ZeRO-2 optimizations and `bf16` precision. \n",
    "\n",
    "The [Transformers documentation](https://huggingface.co/docs/transformers/main_classes/deepspeed#configuration) explains how to write a configuration from scratch very well. A more complete description of all configuration possibilities is can be found in the [DeepSpeed documentation](https://www.deepspeed.ai/docs/config-json/).\n",
    "\n",
    "We create a `ds_config.json` with the below configuration in the `scripts/` directory to be later used for training.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"steps_per_print\": 64,\n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"gradient_clipping\": 1.0,\n",
    "    \"bf16\": {\n",
    "        \"enabled\": true\n",
    "    },\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,\n",
    "        \"overlap_comm\": false,\n",
    "        \"reduce_scatter\": false,\n",
    "        \"contiguous_gradients\": false\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"params\": {\n",
    "            \"lr\": \"auto\",\n",
    "            \"betas\": \"auto\",\n",
    "            \"eps\": \"auto\",\n",
    "            \"weight_decay\": \"auto\"\n",
    "        }\n",
    "    },\n",
    "    \"scheduler\": {\n",
    "        \"type\": \"WarmupLR\",\n",
    "        \"params\": {\n",
    "            \"warmup_min_lr\": \"auto\",\n",
    "            \"warmup_max_lr\": \"auto\",\n",
    "            \"warmup_num_steps\": \"auto\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "> The special value `\"auto\"` enables to automatically get the correct or most efficient value. You can also specify the values yourself but, if you do so, you should be careful to not have conficting values with your training arguments. It is strongly advised to read [this section](https://huggingface.co/docs/transformers/main_classes/deepspeed#shared-configuration) in the Transformers documentation to completely understand how this works.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run T3-3B on Habana Gaudi\n",
    "\n",
    "When using GPUs you would use the [Trainer](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.Trainer) and [TrainingArguments](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.TrainingArguments). Since we are going to run our training on Habana Gaudi we are leveraging the `optimum-habana` library, we can use the [GaudiTrainer](https://huggingface.co/docs/optimum/main/en/habana_trainer) and GaudiTrainingArguments instead. The `GaudiTrainer` is a wrapper around the [Trainer](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.Trainer) that allows you to pre-traing or fine-tune a transformer model on a Habana Gaudi instances.\n",
    "\n",
    "```diff\n",
    "-from transformers import Trainer, TrainingArguments \n",
    "+from optimum.habana import GaudiTrainer, GaudiTrainingArguments\n",
    "\n",
    "# define the training arguments\n",
    "-training_args = TrainingArguments(\n",
    "+training_args = GaudiTrainingArguments(\n",
    "+  use_habana=True,\n",
    "+  use_lazy_mode=True,\n",
    "+  gaudi_config_name=path_to_gaudi_config,\n",
    "  deepspeed=path_to_my_deepspeed_config,\n",
    "  ...\n",
    ")\n",
    "\n",
    "# Initialize our Trainer\n",
    "-trainer = Trainer(\n",
    "+trainer = GaudiTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset\n",
    "    ... # other arguments\n",
    ")\n",
    "```\n",
    "\n",
    "The `DL1` instance we use has 8 available HPU-cores meaning we can leverage distributed data-parallel training for our model. \n",
    "To run our training with `deepspeed` we need to create a training ([scripts/run_summarization.py](https://github.com/philschmid/deep-learning-habana-huggingface/blob/master/pre-training/scripts/run_mlm.py)) implementing our fine-tuning modeling using the `GaudiSeq2SeqTrainer`. To executed our distributed training we use the `DistributedRunner` runner from `optimum-habana` and pass our arguments. Alternatively you could check-out the [gaudi_spawn.py](https://github.com/huggingface/optimum-habana/blob/main/examples/gaudi_spawn.py) in the [optimum-habana](https://github.com/huggingface/optimum-habana) repository.\n",
    "\n",
    "\n",
    "Before we can start our training we need to define the `hyperparameters` we want to use for our training. We are leveraging the [Hugging Face Hub](https://huggingface.co/models) integration of the `GaudiSeq2SeqTrainer` to automatically push our checkpoints, logs and metrics during training into repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# hyperparameters\n",
    "hyperparameters = {\n",
    "    \"model_id\": \"t5-3b\",\n",
    "    \"dataset_id\": \"nickmuchi/trade-the-event-finance\",\n",
    "    \"gaudi_config_id\": \"philschmid/bert-base-uncased-2022-habana\",\n",
    "    \"repository_id\": \"Habana/t5\",\n",
    "    \"hf_hub_token\": HfFolder.get_token(),  # need to be login in with `huggingface-cli login`\n",
    "    \"num_epochs\": 3,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"learning_rate\": 5e-5,\n",
    "}\n",
    "hyperparameters_string = \" \".join(f\"--{key} {value}\" for key, value in hyperparameters.items())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start our training with by creating a `EC2RemoteRunner` and then `launch` it. This will then start our AWS EC2 DL1 instance and runs our `run_mlm.py` script on it using the `huggingface/optimum-habana:latest` container.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-18 11:34:00,286 | INFO | Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2022-10-18 11:34:01,477 | INFO | Created key pair: rm-runner-fggm\n",
      "2022-10-18 11:34:02,272 | INFO | Created security group: rm-runner-fggm\n",
      "2022-10-18 11:34:03,948 | INFO | Launched instance: i-004ce6780377f435d\n",
      "2022-10-18 11:34:03,951 | INFO | Waiting for instance to be ready...\n",
      "2022-10-18 11:34:19,953 | INFO | Instance is ready. Public DNS: ec2-34-238-44-246.compute-1.amazonaws.com\n",
      "2022-10-18 11:34:19,972 | INFO | Setting up ssh connection...\n",
      "2022-10-18 11:35:39,959 | INFO | Setting up ssh connection...\n",
      "2022-10-18 11:36:04,077 | INFO | Setting up ssh connection...\n",
      "2022-10-18 11:36:09,208 | INFO | Setting up ssh connection...\n",
      "2022-10-18 11:36:09,440 | INFO | Connected (version 2.0, client OpenSSH_8.2p1)\n",
      "2022-10-18 11:36:10,849 | INFO | Authentication (publickey) successful!\n",
      "2022-10-18 11:36:10,855 | INFO | Pulling container: huggingface/optimum-habana:4.23.1-pt1.12.0-synapse1.6.0-deepspeed...\n",
      "2022-10-18 11:37:26,871 | INFO | Uploading from scripts\n",
      "2022-10-18 11:37:28,499 | INFO | Executing: docker run --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --net=host --ipc=host -v /home/ubuntu/test:/home/ubuntu/rm-runner --workdir=/home/ubuntu/rm-runner huggingface/optimum-habana:4.23.1-pt1.12.0-synapse1.6.0-deepspeed python3 gaudi_spawn.py --use_deepspeed --world_size=8 run_summarization.py --model_id t5-3b --dataset_id nickmuchi/trade-the-event-finance --gaudi_config_id philschmid/bert-base-uncased-2022-habana --repository_id Habana/t5 --hf_hub_token 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 --num_epochs 3 --per_device_train_batch_size 4 --learning_rate 5e-05  --deepspeed ds_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistributedRunner run(): command = deepspeed --num_nodes 1 --num_gpus 8 --no_local_rank run_summarization.py --model_id t5-3b --dataset_id nickmuchi/trade-the-event-finance --gaudi_config_id philschmid/bert-base-uncased-2022-habana --repository_id Habana/t5 --hf_hub_token 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 --num_epochs 3 --per_device_train_batch_size 4 --learning_rate 5e-05 --deepspeed ds_config.json\n",
      "[2022-10-18 09:37:52,753] [WARNING] [runner.py:159:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "Namespace(autotuning='', exclude='', force_multi=False, hostfile='/job/hostfile', include='', launcher='pdsh', launcher_args='', master_addr='', master_port=29500, module=False, no_local_rank=True, no_python=False, num_gpus=8, num_nodes=1, save_pid=False, use_hpu=False, user_args=['--model_id', 't5-3b', '--dataset_id', 'nickmuchi/trade-the-event-finance', '--gaudi_config_id', 'philschmid/bert-base-uncased-2022-habana', '--repository_id', 'Habana/t5', '--hf_hub_token', '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', '--num_epochs', '3', '--per_device_train_batch_size', '4', '--learning_rate', '5e-05', '--deepspeed', 'ds_config.json'], user_script='run_summarization.py')\n",
      "[2022-10-18 09:37:52,753] [INFO] [runner.py:466:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --no_local_rank run_summarization.py --model_id t5-3b --dataset_id nickmuchi/trade-the-event-finance --gaudi_config_id philschmid/bert-base-uncased-2022-habana --repository_id Habana/t5 --hf_hub_token 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 --num_epochs 3 --per_device_train_batch_size 4 --learning_rate 5e-05 --deepspeed ds_config.json\n",
      "[2022-10-18 09:37:53,307] [INFO] [launch.py:104:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}\n",
      "[2022-10-18 09:37:53,307] [INFO] [launch.py:110:main] nnodes=1, num_local_procs=8, node_rank=0\n",
      "[2022-10-18 09:37:53,307] [INFO] [launch.py:123:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})\n",
      "[2022-10-18 09:37:53,307] [INFO] [launch.py:124:main] dist_world_size=8\n",
      "10/18/2022 09:37:57 - INFO - __main__ - Script parameters ScriptArguments(dataset_id='nickmuchi/trade-the-event-finance', model_id='t5-3b', repository_id='Habana/t5', hf_hub_token='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', gaudi_config_id='philschmid/bert-base-uncased-2022-habana', per_device_train_batch_size=4, num_epochs=3, learning_rate=5e-05, deepspeed='ds_config.json')\n",
      "10/18/2022 09:37:57 - INFO - __main__ - Script parameters ScriptArguments(dataset_id='nickmuchi/trade-the-event-finance', model_id='t5-3b', repository_id='Habana/t5', hf_hub_token='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', gaudi_config_id='philschmid/bert-base-uncased-2022-habana', per_device_train_batch_size=4, num_epochs=3, learning_rate=5e-05, deepspeed='ds_config.json')\n",
      "10/18/2022 09:37:57 - INFO - __main__ - Script parameters ScriptArguments(dataset_id='nickmuchi/trade-the-event-finance', model_id='t5-3b', repository_id='Habana/t5', hf_hub_token='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', gaudi_config_id='philschmid/bert-base-uncased-2022-habana', per_device_train_batch_size=4, num_epochs=3, learning_rate=5e-05, deepspeed='ds_config.json')\n",
      "10/18/2022 09:37:57 - INFO - __main__ - Script parameters ScriptArguments(dataset_id='nickmuchi/trade-the-event-finance', model_id='t5-3b', repository_id='Habana/t5', hf_hub_token='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', gaudi_config_id='philschmid/bert-base-uncased-2022-habana', per_device_train_batch_size=4, num_epochs=3, learning_rate=5e-05, deepspeed='ds_config.json')\n",
      "10/18/2022 09:37:57 - INFO - __main__ - Script parameters ScriptArguments(dataset_id='nickmuchi/trade-the-event-finance', model_id='t5-3b', repository_id='Habana/t5', hf_hub_token='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', gaudi_config_id='philschmid/bert-base-uncased-2022-habana', per_device_train_batch_size=4, num_epochs=3, learning_rate=5e-05, deepspeed='ds_config.json')\n",
      "10/18/2022 09:37:57 - INFO - __main__ - Script parameters ScriptArguments(dataset_id='nickmuchi/trade-the-event-finance', model_id='t5-3b', repository_id='Habana/t5', hf_hub_token='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', gaudi_config_id='philschmid/bert-base-uncased-2022-habana', per_device_train_batch_size=4, num_epochs=3, learning_rate=5e-05, deepspeed='ds_config.json')\n",
      "Downloading metadata: 100%|██████████| 868/868 [00:00<00:00, 1.90MB/s]\n",
      "10/18/2022 09:37:57 - INFO - __main__ - Script parameters ScriptArguments(dataset_id='nickmuchi/trade-the-event-finance', model_id='t5-3b', repository_id='Habana/t5', hf_hub_token='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', gaudi_config_id='philschmid/bert-base-uncased-2022-habana', per_device_train_batch_size=4, num_epochs=3, learning_rate=5e-05, deepspeed='ds_config.json')\n",
      "10/18/2022 09:37:57 - INFO - __main__ - Script parameters ScriptArguments(dataset_id='nickmuchi/trade-the-event-finance', model_id='t5-3b', repository_id='Habana/t5', hf_hub_token='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', gaudi_config_id='philschmid/bert-base-uncased-2022-habana', per_device_train_batch_size=4, num_epochs=3, learning_rate=5e-05, deepspeed='ds_config.json')\n",
      "10/18/2022 09:37:57 - WARNING - datasets.builder - Using custom data configuration nickmuchi--trade-the-event-finance-857419edf4d80858\n",
      "Downloading and preparing dataset json/default (download: 785.12 MiB, generated: 1.40 GiB, post-processed: Unknown size, total: 2.17 GiB) to /root/.cache/huggingface/datasets/nickmuchi___parquet/nickmuchi--trade-the-event-finance-857419edf4d80858/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n",
      "10/18/2022 09:37:57 - WARNING - datasets.builder - Using custom data configuration nickmuchi--trade-the-event-finance-857419edf4d80858\n",
      "10/18/2022 09:37:57 - WARNING - datasets.builder - Using custom data configuration nickmuchi--trade-the-event-finance-857419edf4d80858\n",
      "Downloading metadata:   0%|          | 0.00/868 [00:00<?, ?B/s]10/18/2022 09:37:57 - WARNING - datasets.builder - Using custom data configuration nickmuchi--trade-the-event-finance-857419edf4d80858\n",
      "Downloading metadata: 100%|██████████| 868/868 [00:00<00:00, 1.65MB/s]\n",
      "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]10/18/2022 09:37:57 - WARNING - datasets.builder - Using custom data configuration nickmuchi--trade-the-event-finance-857419edf4d80858\n",
      "10/18/2022 09:37:57 - WARNING - datasets.builder - Using custom data configuration nickmuchi--trade-the-event-finance-857419edf4d80858\n",
      "10/18/2022 09:37:57 - WARNING - datasets.builder - Using custom data configuration nickmuchi--trade-the-event-finance-857419edf4d80858\n",
      "10/18/2022 09:37:57 - WARNING - datasets.builder - Using custom data configuration nickmuchi--trade-the-event-finance-857419edf4d80858\n",
      "\n",
      "Downloading data:   0%|          | 0.00/124M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  10%|▉         | 12.3M/124M [00:00<00:00, 123MB/s]\u001b[A\n",
      "Downloading data:  20%|██        | 25.1M/124M [00:00<00:00, 126MB/s]\u001b[A\n",
      "Downloading data:  31%|███       | 38.0M/124M [00:00<00:00, 127MB/s]\u001b[A\n",
      "Downloading data:  41%|████      | 50.8M/124M [00:00<00:00, 127MB/s]\u001b[A\n",
      "Downloading data:  51%|█████▏    | 63.5M/124M [00:00<00:00, 127MB/s]\u001b[A\n",
      "Downloading data:  62%|██████▏   | 76.2M/124M [00:00<00:00, 126MB/s]\u001b[A\n",
      "Downloading data:  72%|███████▏  | 88.8M/124M [00:00<00:00, 125MB/s]\u001b[A\n",
      "Downloading data:  82%|████████▏ | 101M/124M [00:00<00:00, 125MB/s] \u001b[A\n",
      "Downloading data: 100%|██████████| 124M/124M [00:00<00:00, 125MB/s]\u001b[A\n",
      "Downloading data files:  50%|█████     | 1/2 [00:01<00:01,  1.05s/it]\n",
      "Downloading data:   0%|          | 0.00/233M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   5%|▌         | 11.8M/233M [00:00<00:01, 118MB/s]\u001b[A\n",
      "Downloading data:  10%|█         | 24.4M/233M [00:00<00:01, 122MB/s]\u001b[A\n",
      "Downloading data:  16%|█▌        | 37.0M/233M [00:00<00:01, 124MB/s]\u001b[A\n",
      "Downloading data:  21%|██▏       | 49.7M/233M [00:00<00:01, 125MB/s]\u001b[A\n",
      "Downloading data:  27%|██▋       | 62.4M/233M [00:00<00:01, 126MB/s]\u001b[A\n",
      "Downloading data:  32%|███▏      | 75.3M/233M [00:00<00:01, 127MB/s]\u001b[A\n",
      "Downloading data:  38%|███▊      | 88.2M/233M [00:00<00:01, 128MB/s]\u001b[A\n",
      "Downloading data:  44%|████▎     | 101M/233M [00:00<00:01, 129MB/s] \u001b[A\n",
      "Downloading data:  49%|████▉     | 114M/233M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  55%|█████▍    | 127M/233M [00:01<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  60%|██████    | 140M/233M [00:01<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  66%|██████▌   | 153M/233M [00:01<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  71%|███████▏  | 166M/233M [00:01<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  77%|███████▋  | 179M/233M [00:01<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  83%|████████▎ | 192M/233M [00:01<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  88%|████████▊ | 205M/233M [00:01<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  94%|█████████▎| 218M/233M [00:01<00:00, 129MB/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 233M/233M [00:01<00:00, 128MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/234M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   5%|▌         | 12.5M/234M [00:00<00:01, 125MB/s]\u001b[A\n",
      "Downloading data:  11%|█         | 25.3M/234M [00:00<00:01, 127MB/s]\u001b[A\n",
      "Downloading data:  16%|█▋        | 38.3M/234M [00:00<00:01, 128MB/s]\u001b[A\n",
      "Downloading data:  22%|██▏       | 51.3M/234M [00:00<00:01, 129MB/s]\u001b[A\n",
      "Downloading data:  28%|██▊       | 64.3M/234M [00:00<00:01, 129MB/s]\u001b[A\n",
      "Downloading data:  33%|███▎      | 77.3M/234M [00:00<00:01, 130MB/s]\u001b[A\n",
      "Downloading data:  39%|███▊      | 90.3M/234M [00:00<00:01, 130MB/s]\u001b[A\n",
      "Downloading data:  44%|████▍     | 103M/234M [00:00<00:01, 129MB/s] \u001b[A\n",
      "Downloading data:  50%|████▉     | 116M/234M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  55%|█████▌    | 129M/234M [00:01<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  61%|██████    | 142M/234M [00:01<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  66%|██████▋   | 155M/234M [00:01<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  72%|███████▏  | 168M/234M [00:01<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  78%|███████▊  | 181M/234M [00:01<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  83%|████████▎ | 194M/234M [00:01<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  89%|████████▊ | 207M/234M [00:01<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  94%|█████████▍| 220M/234M [00:01<00:00, 130MB/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 234M/234M [00:01<00:00, 129MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/234M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   5%|▌         | 12.4M/234M [00:00<00:01, 124MB/s]\u001b[A\n",
      "Downloading data:  11%|█         | 25.3M/234M [00:00<00:01, 127MB/s]\u001b[A\n",
      "Downloading data:  16%|█▋        | 38.1M/234M [00:00<00:01, 127MB/s]\u001b[A\n",
      "Downloading data:  22%|██▏       | 51.1M/234M [00:00<00:01, 128MB/s]\u001b[A\n",
      "Downloading data:  27%|██▋       | 64.1M/234M [00:00<00:01, 129MB/s]\u001b[A\n",
      "Downloading data:  33%|███▎      | 77.1M/234M [00:00<00:01, 126MB/s]\u001b[A\n",
      "Downloading data:  38%|███▊      | 89.7M/234M [00:00<00:01, 124MB/s]\u001b[A\n",
      "Downloading data:  44%|████▎     | 102M/234M [00:00<00:01, 123MB/s] \u001b[A\n",
      "Downloading data:  49%|████▉     | 114M/234M [00:00<00:00, 121MB/s]\u001b[A\n",
      "Downloading data:  54%|█████▍    | 127M/234M [00:01<00:00, 121MB/s]\u001b[A\n",
      "Downloading data:  59%|█████▉    | 139M/234M [00:01<00:00, 120MB/s]\u001b[A\n",
      "Downloading data:  64%|██████▍   | 151M/234M [00:01<00:00, 119MB/s]\u001b[A\n",
      "Downloading data:  70%|██████▉   | 163M/234M [00:01<00:00, 122MB/s]\u001b[A\n",
      "Downloading data:  75%|███████▌  | 176M/234M [00:01<00:00, 124MB/s]\u001b[A\n",
      "Downloading data:  81%|████████  | 189M/234M [00:01<00:00, 125MB/s]\u001b[A\n",
      "Downloading data:  86%|████████▋ | 202M/234M [00:01<00:00, 126MB/s]\u001b[A\n",
      "Downloading data:  92%|█████████▏| 215M/234M [00:01<00:00, 127MB/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 234M/234M [00:01<00:00, 125MB/s]\u001b[A\n",
      "Downloading data files: 100%|██████████| 2/2 [00:06<00:00,  3.38s/it]\n",
      "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 787.88it/s]\n",
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/nickmuchi___parquet/nickmuchi--trade-the-event-finance-857419edf4d80858/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]10/18/2022 09:38:10 - WARNING - datasets.builder - Found cached dataset parquet (/root/.cache/huggingface/datasets/nickmuchi___parquet/nickmuchi--trade-the-event-finance-857419edf4d80858/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 2/2 [00:00<00:00, 583.84it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]10/18/2022 09:38:10 - WARNING - datasets.builder - Found cached dataset parquet (/root/.cache/huggingface/datasets/nickmuchi___parquet/nickmuchi--trade-the-event-finance-857419edf4d80858/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 2/2 [00:00<00:00, 843.08it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 705.52it/s]\n",
      "10/18/2022 09:38:10 - WARNING - datasets.builder - Found cached dataset parquet (/root/.cache/huggingface/datasets/nickmuchi___parquet/nickmuchi--trade-the-event-finance-857419edf4d80858/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 2/2 [00:00<00:00, 650.33it/s]\n",
      "10/18/2022 09:38:10 - WARNING - datasets.builder - Found cached dataset parquet (/root/.cache/huggingface/datasets/nickmuchi___parquet/nickmuchi--trade-the-event-finance-857419edf4d80858/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 2/2 [00:00<00:00, 931.76it/s]\n",
      "10/18/2022 09:38:10 - WARNING - datasets.builder - Found cached dataset parquet (/root/.cache/huggingface/datasets/nickmuchi___parquet/nickmuchi--trade-the-event-finance-857419edf4d80858/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 2/2 [00:00<00:00, 885.72it/s]\n",
      "10/18/2022 09:38:10 - WARNING - datasets.builder - Found cached dataset parquet (/root/.cache/huggingface/datasets/nickmuchi___parquet/nickmuchi--trade-the-event-finance-857419edf4d80858/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 2/2 [00:00<00:00, 655.87it/s]\n",
      "10/18/2022 09:38:10 - WARNING - datasets.builder - Found cached dataset parquet (/root/.cache/huggingface/datasets/nickmuchi___parquet/nickmuchi--trade-the-event-finance-857419edf4d80858/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 2/2 [00:00<00:00, 884.13it/s]\n",
      "Downloading: 100%|██████████| 1.20k/1.20k [00:00<00:00, 2.13MB/s]\n",
      "Downloading: 100%|██████████| 792k/792k [00:00<00:00, 113MB/s]\n",
      "Downloading: 100%|██████████| 1.39M/1.39M [00:00<00:00, 114MB/s]\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-3b automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-3b automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-3b automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-3b automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-3b automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-3b automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-3b automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-3b automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/46 [00:00<?, ?ba/s]0:00<?, ?ba/s]/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      " 98%|█████████▊| 45/46 [00:53<00:01,  1.19s/ba]\n",
      " 98%|█████████▊| 45/46 [00:53<00:01,  1.20s/ba]\n",
      " 98%|█████████▊| 45/46 [00:55<00:01,  1.24s/ba]\n",
      " 98%|█████████▊| 45/46 [00:56<00:01,  1.26s/ba]\n",
      " 98%|█████████▊| 45/46 [00:57<00:01,  1.28s/ba]\n",
      " 98%|█████████▊| 45/46 [00:58<00:01,  1.29s/ba]\n",
      " 98%|█████████▊| 45/46 [00:59<00:01,  1.32s/ba]\n",
      " 98%|█████████▊| 45/46 [00:59<00:01,  1.33s/ba]\n",
      " 59%|█████▉    | 153/259 [03:16<02:17,  1.30s/ba]"
     ]
    }
   ],
   "source": [
    "from rm_runner import EC2RemoteRunner\n",
    "# create ec2 remote runner\n",
    "runner = EC2RemoteRunner(\n",
    "  instance_type=\"dl1.24xlarge\",\n",
    "  profile=\"hf-sm\",  # adjust to your profile\n",
    "  region=\"us-east-1\",\n",
    "  container=image_id # defined in the first step\n",
    "  )\n",
    "\n",
    "# launch my script with gaudi_spawn for the deepspeed training\n",
    "runner.launch(\n",
    "    command=f\"python3 gaudi_spawn.py --use_deepspeed --world_size=8 run_summarization.py {hyperparameters_string}  --deepspeed ds_config.json\",\n",
    "    source_dir=\"scripts\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3.8.12 ('hf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5fcf248a74081676ead7e77f54b2c239ba2921b952f7cbcdbbe5427323165924"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
