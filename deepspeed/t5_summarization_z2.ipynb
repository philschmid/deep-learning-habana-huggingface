{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use DeepSpeed with Habana Gaudi for large-scale efficitent training\n",
    "\n",
    "In this blog, you will learn how to fine-tune [T5-3B](https://huggingface.co/t5-3b) for abstractive summarization with [DeepSpeed](https://www.deepspeed.ai/) using a Habana Gaudi-based [DL1 instance](https://aws.amazon.com/ec2/instance-types/dl1/) on AWS to take advantage of the cost performance benefits of Gaudi. We will use the Hugging Faces Transformers, Optimum Habana and Datasets library as well as the Habana fork of [DeepSpeed](https://github.com/HabanaAI/DeepSpeed). We are going to fine-tune fine-tune [T5-3B](https://huggingface.co/t5-3b) using the [Trade the Event](https://paperswithcode.com/paper/trade-the-event-corporate-events-detection) dataset for abstractive text summarization. The benchmark dataset contains 303893 news articles range from 2020/03/01 to 2021/05/06. The articles are downloaded from the [PRNewswire](https://www.prnewswire.com/) and [Businesswire](https://www.businesswire.com/).\n",
    "\n",
    "\n",
    "You will learn how to:\n",
    "\n",
    "1. [Setup Cloud Environment](#1-setup-cloud-environment)\n",
    "2. [Configure DeepSpeed](#2-configure-deepspeed)\n",
    "3. [Run T3-3B on Habana Gaudi](#3-run-t3-3b-on-habana-gaudi)\n",
    "4. [Cost performance benefits of Habana Gaudi on AWS](#4-cost-performance-benefits-of-habana-gaudi-on-aws)\n",
    "\n",
    "![remote-runner](../assets/remote-runner.png)\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "Before we can start, make sure you have met the following requirements\n",
    "\n",
    "* AWS Account with quota for [DL1 instance type](https://aws.amazon.com/ec2/instance-types/dl1/)\n",
    "* [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) installed\n",
    "* AWS IAM user [configured in CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) with permission to create and manage ec2 instances\n",
    "\n",
    "**Helpful Resources**\n",
    "\n",
    "* [Setup Deep Learning environment for Hugging Face Transformers with Habana Gaudi on AWS](https://www.philschmid.de/getting-started-habana-gaudi)\n",
    "* [Deep Learning setup made easy with EC2 Remote Runner and Habana Gaudi](https://www.philschmid.de/habana-gaudi-ec2-runner)\n",
    "* [Optimum Habana Documentation](https://huggingface.co/docs/optimum/main/en/habana_index)\n",
    "* [Pre-Training BERT with Hugging Face Transformers and Habana Gaudi](https://www.philschmid.de/pre-training-bert-habana)\n",
    "* [Habana DeepSpeed User Guide](https://docs.habana.ai/en/latest/PyTorch/DeepSpeed/DeepSpeed_User_Guide.html)\n",
    "\n",
    "\n",
    "## What is DeepSpeed? \n",
    "\n",
    "[DeepSpeed](https://www.deepspeed.ai/training/) is an easy-to-use deep learning optimization software suite that enables unprecedented scale and speed for Deep Learning Training. [DeepSpeed](https://www.deepspeed.ai/) enables you to fit and train larger models on HPUs thanks to various optimizations. In particular, you can use the two following ZeRO configurations that have been validated to be fully functioning with Gaudi:\n",
    "\n",
    "* ZeRO-1, which partitions the optimizer states across processes.\n",
    "* ZeRO-2, which additionnally partitions the gradients across processes so that each process retains only the gradients corresponding to its portion of the optimizer states.\n",
    "\n",
    "These configurations are fully compatible with Habana Mixed Precision and can thus be used to train your model in bf16 precision.\n",
    "\n",
    "You can find more information about DeepSpeed Gaudi integration [here](https://huggingface.co/docs/optimum/habana_deepspeed).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import habana_frameworks.torch.hpu as hthpu\n",
    "hthpu.is_available()\n",
    "hthpu.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Cloud Environment\n",
    "\n",
    "In this example are we going to use Habana Gaudi on AWS using the DL1 instance for running the training. We will use the [create_aws_instance.sh](./create_aws_instance.sh) script to create a DL1 instance using the Optimum Habana AMI. The AMI is currently available in `us-east-1` & `us-west-2`. \n",
    "The Optimum Habana AMI comes with the following pre-installed:\n",
    "* tensorboard\n",
    "* transformers==4.23.1\n",
    "* datasets==2.6.1\n",
    "* evaluate==0.3.0\n",
    "* optimum==1.4.0\n",
    "* optimum-habana==1.2.3\n",
    "* rouge-score\n",
    "* nltk\n",
    "* https://github.com/HabanaAI/DeepSpeed # Fork of deepspeed\n",
    "\n",
    "First step is to clone the github repository and change into the directory:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/philschmid/deep-learning-habana-huggingface.git\n",
    "\n",
    "cd deepspeed\n",
    "```\n",
    "\n",
    "Now we can start the instance using the `create_aws_instance.sh` script. The `create_aws_instance.sh` script will create:\n",
    "* a `ssh` key pair\n",
    "* a security group in your default VPC\n",
    "* a DL1 instance with the Optimum Habana AMI\n",
    "\n",
    "```bash\n",
    "./scripts/01_create_aws_instance.sh\n",
    "```\n",
    "\n",
    "The next steps is copy our local jupyter notebook to the instance and start the jupyter notebook server. We will use the [start_jupyter.sh](./scripts/02_start_jupyter.sh) script, which takes care of copying to the AWS instance. The script will also start a jupyter notebook server on Gaudi instance, which we can by opening the \"`localhost`\" link in the terminal. \n",
    "\n",
    "```bash\n",
    "./scripts/02_start_jupyter.sh\n",
    "```\n",
    "\n",
    "example output below\n",
    "```bash\n",
    "[W 15:10:42.614 NotebookApp] No web browser found: could not locate runnable browser.\n",
    "[C 15:10:42.614 NotebookApp] \n",
    "    \n",
    "    To access the notebook, open this file in a browser:\n",
    "        file:///home/ubuntu/.local/share/jupyter/runtime/nbserver-6385-open.html\n",
    "    Or copy and paste one of these URLs:\n",
    "        http://localhost:4753/?token=61913a231e47dea5fdb40720be30ae2f7f6bed90d21d7382\n",
    "     or http://127.0.0.1:4753/?token=61913a231e47dea5fdb40720be30ae2f7f6bed90d21d7382\n",
    "```\n",
    "\n",
    "Now, we can open the `t5_summarization_z2.ipynb` jupyter notebook in our browser and start the with the deepspeed training.\n",
    "\n",
    "Lets test if we can access our `HPUs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import habana_frameworks.torch.hpu as hthpu\n",
    "hthpu.is_available()\n",
    "hthpu.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example will use the [Hugging Face Hub](https://huggingface.co/models) as a remote model versioning service. To be able to push our model to the Hub, you need to register on the [Hugging Face](https://huggingface.co/join). \n",
    "If you already have an account you can skip this step. \n",
    "After you have an account, we will use the `notebook_login` util from the `huggingface_hub` package to log into our account and store our token (access key) on the disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure DeepSpeed\n",
    "\n",
    "The [GaudiTrainer](https://huggingface.co/docs/optimum/main/en/habana_trainer) allows us to use DeepSpeed as easily as the [Transformers Trainer](https://huggingface.co/docs/transformers/main_classes/trainer). It will take care of adding all of the DeepSpeed specific methods and checks. To add DeepSpeed to our training we have to: \n",
    "\n",
    "1. Create a DeepSpeed configuration\n",
    "2. Specify the DeepSpeed configuration in the `GaudiTrainer`\n",
    "3. Use `deepspeed` to launch our training\n",
    "   \n",
    "These steps are detailed below. A comprehensive guide about how to use DeepSpeed with the Transformers Trainer is also available [here](https://huggingface.co/docs/transformers/main_classes/deepspeed).\n",
    "\n",
    "\n",
    "### 1. Create DeepSpeed configuration\n",
    "\n",
    "The DeepSpeed configuration is passed through as JSON file and enables you to choose the optimizations to apply. Below you will find the confiugration we are going to use for fine-tuning `T5-3b` using ZeRO-2 optimizations and `bf16` precision. \n",
    "\n",
    "The [Transformers documentation](https://huggingface.co/docs/transformers/main_classes/deepspeed#configuration) explains how to write a configuration from scratch very well. A more complete description of all configuration possibilities is can be found in the [DeepSpeed documentation](https://www.deepspeed.ai/docs/config-json/).\n",
    "\n",
    "We create a `ds_config.json` with the below configuration in the `src/` directory to be later used for training.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"steps_per_print\": 64,\n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"gradient_clipping\": 1.0,\n",
    "    \"bf16\": {\n",
    "        \"enabled\": true\n",
    "    },\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,\n",
    "        \"overlap_comm\": false,\n",
    "        \"reduce_scatter\": false,\n",
    "        \"contiguous_gradients\": false\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"params\": {\n",
    "            \"lr\": \"auto\",\n",
    "            \"betas\": \"auto\",\n",
    "            \"eps\": \"auto\",\n",
    "            \"weight_decay\": \"auto\"\n",
    "        }\n",
    "    },\n",
    "    \"scheduler\": {\n",
    "        \"type\": \"WarmupLR\",\n",
    "        \"params\": {\n",
    "            \"warmup_min_lr\": \"auto\",\n",
    "            \"warmup_max_lr\": \"auto\",\n",
    "            \"warmup_num_steps\": \"auto\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "> The special value `\"auto\"` enables to automatically get the correct or most efficient value. You can also specify the values yourself but, if you do so, you should be careful to not have conficting values with your training arguments. It is strongly advised to read [this section](https://huggingface.co/docs/transformers/main_classes/deepspeed#shared-configuration) in the Transformers documentation to completely understand how this works.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run T3-3B on Habana Gaudi\n",
    "\n",
    "When using GPUs you would use the [Trainer](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.Trainer) and [TrainingArguments](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.TrainingArguments). Since we are going to run our training on Habana Gaudi we are leveraging the `optimum-habana` library, we can use the [GaudiTrainer](https://huggingface.co/docs/optimum/main/en/habana_trainer) and GaudiTrainingArguments instead. The `GaudiTrainer` is a wrapper around the [Trainer](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.Trainer) that allows you to pre-traing or fine-tune a transformer model on a Habana Gaudi instances.\n",
    "\n",
    "```diff\n",
    "-from transformers import Trainer, TrainingArguments \n",
    "+from optimum.habana import GaudiTrainer, GaudiTrainingArguments\n",
    "\n",
    "# define the training arguments\n",
    "-training_args = TrainingArguments(\n",
    "+training_args = GaudiTrainingArguments(\n",
    "+  use_habana=True,\n",
    "+  use_lazy_mode=True,\n",
    "+  gaudi_config_name=path_to_gaudi_config,\n",
    "  deepspeed=path_to_my_deepspeed_config,\n",
    "  ...\n",
    ")\n",
    "\n",
    "# Initialize our Trainer\n",
    "-trainer = Trainer(\n",
    "+trainer = GaudiTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset\n",
    "    ... # other arguments\n",
    ")\n",
    "```\n",
    "\n",
    "The `DL1` instance we use has 8 available HPU-cores meaning we can leverage distributed data-parallel training for our model. \n",
    "To run our training with `deepspeed` we need to create a training ([src/run_summarization.py](https://github.com/philschmid/deep-learning-habana-huggingface/blob/master/deepspeed/src/run_summiarzation.py)) implementing our fine-tuning modeling using the `GaudiSeq2SeqTrainer`. To executed our distributed training we use the `DistributedRunner` runner from `optimum-habana` and pass our arguments. Alternatively you could check-out the [gaudi_spawn.py](https://github.com/huggingface/optimum-habana/blob/main/examples/gaudi_spawn.py) in the [optimum-habana](https://github.com/huggingface/optimum-habana) repository.\n",
    "\n",
    "\n",
    "Before we can start our training we need to define the `hyperparameters` we want to use for our training. We are leveraging the [Hugging Face Hub](https://huggingface.co/models) integration of the `GaudiSeq2SeqTrainer` to automatically push our checkpoints, logs and metrics during training into repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# hyperparameters\n",
    "hyperparameters = {\n",
    "    \"model_id\": \"t5-3b\",\n",
    "    \"dataset_id\": \"nickmuchi/trade-the-event-finance\",\n",
    "    \"gaudi_config_id\": \"philschmid/bert-base-uncased-2022-habana\",\n",
    "    \"repository_id\": \"Habana/t5\",\n",
    "    \"hf_hub_token\": HfFolder.get_token(),  # need to be login in with `huggingface-cli login`\n",
    "    \"num_epochs\": 3,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"learning_rate\": 5e-5,\n",
    "}\n",
    "hyperparameters_string = \" \".join(f\"--{key} {value}\" for key, value in hyperparameters.items())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can launch our training using the `DistributedRunner`. The `DistributedRunner` uses the `deepspeed` launcher under the hood to run our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.habana.distributed import DistributedRunner\n",
    "\n",
    "world_size=8 # Number of HPUs to use (1 or 8)\n",
    "\n",
    "# define distributed runner\n",
    "distributed_runner = DistributedRunner(\n",
    "    command_list=[f\"src/run_summarization.py {hyperparameters_string} --deepspeed src/ds_config.json\"],\n",
    "    world_size=world_size,\n",
    "    use_deepspeed=True,\n",
    "    multi_hls=False,\n",
    ")\n",
    "\n",
    "# start job\n",
    "distributed_runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After our training is finished we have to make sure to clean up our cloud resources again. There for we can run the `delete_aws_instances.sh` script.\n",
    "\n",
    "```bash\n",
    "./scripts/03_delete_aws_instance.sh\n",
    "```\n",
    "\n",
    "The script will terminate the `DL1` instance and delete the security group.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cost performance benefits of Habana Gaudi on AWS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3.8.12 ('hf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5fcf248a74081676ead7e77f54b2c239ba2921b952f7cbcdbbe5427323165924"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
