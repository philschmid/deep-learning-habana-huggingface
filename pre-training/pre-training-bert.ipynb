{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Training BERT with Hugging Face Transformers and Habana Gaudi\n",
    "\n",
    "In this tutorial, you will learn how to pre-train [BERT-base](https://huggingface.co/bert-base-uncased) from scratch using a Habana Gaudi-based [DL1 instance](https://aws.amazon.com/ec2/instance-types/dl1/) on AWS to take advantage of the cost performance benefits of Gaudi. We will use the Hugging Faces [Transformers](https://huggingface.co/docs/transformers), [Optimum Habana](https://huggingface.co/docs/optimum/main/en/habana_index) and [Datasets](https://huggingface.co/docs/datasets) library to pre-train a BERT-base model using masked-language modelling one of the two original BERT pre-training tasks. Before we get started, we need to set up the deep learning environment.\n",
    "\n",
    "You will learn how to:\n",
    "\n",
    "1. [Prepare the dataset](#1-prepare-the-dataset)\n",
    "2. [Train a Tokenizer](#2-train-a-tokenizer)\n",
    "3. [Preprocess the dataset](#3-preprocess-the-dataset)\n",
    "4. [Pre-train BERT on Habana Gaudi](#4-pre-train-bert-on-habana-gaudi)\n",
    "\n",
    "_Note: Step 1 to 3 can/should be run on a different instance size those are CPU intensive tasks._\n",
    "\n",
    "![pre-training overview](../assets/pre-training.png)\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "Before we can start make sure you have met the following requirements\n",
    "\n",
    "* AWS Account with quota for [DL1 instance type](https://aws.amazon.com/ec2/instance-types/dl1/)\n",
    "* [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) installed\n",
    "* AWS IAM user [configured in CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) with permission to create and manage ec2 instances\n",
    "\n",
    "**Helpful Resources**\n",
    "\n",
    "* [Setup Deep Learning environment for Hugging Face Transformers with Habana Gaudi on AWS](https://www.philschmid.de/getting-started-habana-gaudi)\n",
    "Gaudi on AWS](https://www.philschmid.de/getting-started-habana-gaudi)\n",
    "* [Deep Learning setup made easy with EC2 Remote Runner and Habana Gaudi](https://www.philschmid.de/habana-gaudi-ec2-runner)\n",
    "* [Optimum Habana Documentation](https://huggingface.co/docs/optimum/main/en/habana_index)\n",
    "* [Pre-training script](./scripts/run_mlm.py)\n",
    "\n",
    "\n",
    "\n",
    "## What is BERT? \n",
    "\n",
    "BERT, short for Bidirectional Encoder Representations from Transformers, is a Machine Learning (ML) model for natural language processing. It was developed in 2018 by researchers at Google AI Language and serves as a swiss army knife solution to 11+ of the most common language tasks, such as sentiment analysis and named entity recognition.\n",
    "\n",
    "Read more about BERT in our [BERT 101 ü§ó State Of The Art NLP Model Explained](https://huggingface.co/blog/bert-101) blog.\n",
    "\n",
    "## What is a Masked Language Modeling (MLM)?\n",
    "\n",
    "MLM enables/enforces bidirectional learning from text by masking (hiding) a word in a sentence and forcing BERT to bidirectionally use the words on either side of the covered word to predict the masked word.\n",
    "\n",
    "**Masked Language Modeling Example:**\n",
    "\n",
    "```bash\n",
    "‚ÄúDang! I‚Äôm out fishing and a huge trout just [MASK] my line!‚Äù\n",
    "```\n",
    "Read more about Masked Language Modeling [here](https://huggingface.co/blog/bert-101).\n",
    "\n",
    "--- \n",
    "\n",
    "Lets get started. üöÄ\n",
    "\n",
    "_Note: Step 1 to 3 where run on a AWS c6i.12xlarge instance._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare the dataset\n",
    "\n",
    "The Tutorial is \"split\" into two parts. The first part (step 1-3) is about preparing the dataset and tokenizer. The second part (step 4) is about pre-training BERT on the prepared dataset. Before we can start with the dataset preparation we need to setup our development environment. As mentioned in the introduction you don't need to prepare the dataset on the DL1 instance and could use your notebook or desktop computer. \n",
    "\n",
    "As first we are going to install `transformers`, `datsets` and `git-lfs` to push our Tokenizer and dataset to the [Hugging Face Hub](https://huggingface.co) for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets\n",
    "!sudo apt-get install git-lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to finish our setup lets log into the [Hugging Face Hub](https://huggingface.co/models) to push our dataset, tokenizer, model artifacts, logs and metrics during training and afterwards to the hub. \n",
    "\n",
    "_To be able to push our model to the Hub, you need to register on the [Hugging Face](https://huggingface.co/join)._\n",
    "\n",
    "We will use the `notebook_login` util from the `huggingface_hub` package to log into our account. You can get your token in the settings at [Access Tokens](https://huggingface.co/settings/tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0567f2ff4ae486bbf97cd09cdcb3ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are now logged in lets get the `user_id`, which will be used to push the artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user id 'philschmid' will be used during the example\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "user_id = HfApi().whoami()[\"name\"]\n",
    "\n",
    "print(f\"user id '{user_id}' will be used during the example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [original BERT](https://arxiv.org/abs/1810.04805) was pretrained on [Wikipedia](https://huggingface.co/datasets/wikipedia) and [BookCorpus](https://huggingface.co/datasets/bookcorpus) dataset. Both datasets are available on the [Hugging Face Hub](https://huggingface.co/datasets) and can be loaded with `datasets`. \n",
    "\n",
    "_Note: For wikipedia we will use the `20220301`, which is different to the original split._\n",
    "\n",
    "As a first step are we loading the dataset and merging them together to create on big dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset bookcorpus (/home/ubuntu/.cache/huggingface/datasets/bookcorpus/plain_text/1.0.0/44662c4a114441c35200992bea923b170e6f13f2f0beb7c14e43759cec498700)\n",
      "Reusing dataset wikipedia (/home/ubuntu/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559)\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets, load_dataset\n",
    "\n",
    "bookcorpus = load_dataset(\"bookcorpus\", split=\"train\")\n",
    "wiki = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\")\n",
    "wiki = wiki.remove_columns([col for col in wiki.column_names if col != \"text\"])  # only keep the 'text' column\n",
    "\n",
    "assert bookcorpus.features.type == wiki.features.type\n",
    "raw_datasets = concatenate_datasets([bookcorpus, wiki])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 80462898\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We are not going to do some advanced dataset preparation, like de-duplication, filtering or any other pre-processing. If you are planning to apply this notebook to train your own BERT model from scratch I highly recommend to including those data preparation steps into your workflow. This will help you improve your Language Model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train a Tokenizer\n",
    "\n",
    "To be able to train our model we need to convert our text into a tokenized format. Most Transformer models are coming with a pre-trained tokenizer, but since we are pre-training our model from scratch we also need to train a Tokenizer on our data. We can train a tokenizer on our data with `transformers` and the `BertTokenizerFast` class. \n",
    "\n",
    "More information about training a new tokenizer can be found in our [Hugging Face Course](https://huggingface.co/course/chapter6/2?fw=pt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# repositor id for saving the tokenizer\n",
    "tokenizer_id=\"bert-base-uncased-2022-habana\"\n",
    "\n",
    "# create a python generator to dynamically load the data\n",
    "def batch_iterator(batch_size=10000):\n",
    "    for i in tqdm(range(0, len(raw_datasets), batch_size)):\n",
    "        yield raw_datasets[i : i + batch_size][\"text\"]\n",
    "\n",
    "# create a tokenizer from existing one to re-use special tokens\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start training the tokenizer with `train_new_from_iterator()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8047/8047 [09:13<00:00, 14.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('tokenizer/tokenizer_config.json',\n",
       " 'tokenizer/special_tokens_map.json',\n",
       " 'tokenizer/vocab.txt',\n",
       " 'tokenizer/added_tokens.json',\n",
       " 'tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer = tokenizer.train_new_from_iterator(text_iterator=batch_iterator(), vocab_size=32_000)\n",
    "bert_tokenizer.save_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We push the tokenizer to [Hugging Face Hub](https://huggingface.co/models) for later training our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you need to be logged into push the tokenizer\n",
    "bert_tokenizer.push_to_hub(tokenizer_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocess the dataset\n",
    "\n",
    "The last step before we can get started with training our model is to pre-process/tokenize our dataset. We will use our trained tokenizer to tokenize our dataset and then push it to hub to load it easily later in our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max length for the tokenizer is: 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25a86c14dd8e44ef951383bfa75578b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80463 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (594 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'special_tokens_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import multiprocessing\n",
    "\n",
    "# load tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(f\"{user_id}/{tokenizer_id}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer\")\n",
    "num_proc = multiprocessing.cpu_count()\n",
    "print(f\"The max length for the tokenizer is: {tokenizer.model_max_length}\")\n",
    "\n",
    "def group_texts(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "       examples[\"text\"], return_special_tokens_mask=True, truncate=True, max_length=tokenizer.model_max_length\n",
    "    )\n",
    "    return tokenized_inputs\n",
    "\n",
    "# preprocess dataset\n",
    "tokenized_datasets = raw_datasets.map(group_texts, batched=True, remove_columns=[\"text\"], num_proc=num_proc)\n",
    "tokenized_datasets.features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As data processing function will we concatenate all texts from our dataset and generate chunks of `tokenizer.model_max_length` (512)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
    "# max_seq_length.\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= tokenizer.model_max_length:\n",
    "        total_length = (total_length // tokenizer.model_max_length) * tokenizer.model_max_length\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + tokenizer.model_max_length] for i in range(0, total_length, tokenizer.model_max_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.map(group_texts, batched=True, num_proc=num_proc)\n",
    "\n",
    "print(f\"the dataset contains in total {len(tokenized_datasets)*tokenizer.model_max_length} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step before we can start with out training is to push our prepared dataset to the hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03cfdd3a9d4843a18f79482cec8bed8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.9/site-packages/huggingface_hub/hf_api.py:1948: FutureWarning: `identical_ok` has no effect and is deprecated. It will be removed in 0.11.0.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# push dataset to hugging face\n",
    "dataset_id=f\"{user_id}/processed_bert_dataset\"\n",
    "tokenized_datasets.push_to_hub(f\"{user_id}/processed_bert_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Setup Habana Gaudi instance\n",
    "\n",
    "> The following steps are created to be executed on a Gaudi instance.\n",
    "\n",
    "In this example are we going to use Habana Gaudi on AWS using the DL1 instance. We already have created a blog post in the past on how to [Setup Deep Learning environment for Hugging Face Transformers with Habana Gaudi on AWS](https://www.philschmid.de/getting-started-habana-gaudi). If you haven't have read this blog post, please read it first and go through the steps on how to setup the environment. \n",
    "Or if you feel comfortable you can use the `start_instance.sh` in the root of the repository to create your DL1 instance and the continue at step  [4. Use Jupyter Notebook/Lab via ssh](https://www.philschmid.de/getting-started-habana-gaudi#4-use-jupyter-notebooklab-via-ssh) in the Setup blog post.\n",
    "\n",
    "1. run habana docker container an mount current directory\n",
    "```bash\n",
    "docker run -ti --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --net=host --ipc=host -v $(pwd):/home/ubuntu/dev --workdir=/home/ubuntu/dev vault.habana.ai/gaudi-docker/1.4.1/ubuntu20.04/habanalabs/pytorch-installer-1.10.2:1.4.1-11\n",
    "```\n",
    "2. install juptyer\n",
    "```bash\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "3. clone repository\n",
    "```bash\n",
    "git clone https://github.com/philschmid/deep-learning-habana-huggingface.git\n",
    "cd fine-tuning\n",
    "```\n",
    "\n",
    "4. run jupyter notebook\n",
    "```bash\n",
    "jupyter notebook --allow-root\n",
    "#         http://localhost:8888/?token=f8d00db29a6adc03023413b7f234d110fe0d24972d7ae65e\n",
    "```\n",
    "4. continue here\n",
    "\n",
    "_**NOTE**: The following steps assume that the code/cells are running on a gaudi instance with access to HPUs_\n",
    "\n",
    "As first lets make sure we have access to the HPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device available:True\n",
      "device_count:8\n"
     ]
    }
   ],
   "source": [
    "import habana_frameworks.torch.core as htcore\n",
    "\n",
    "print(f\"device available:{htcore.is_available()}\")\n",
    "print(f\"device_count:{htcore.get_device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pre-train BERT on Habana Gaudi\n",
    "\n",
    "Normally you would use the [Trainer](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.Trainer) and [TrainingArguments](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.TrainingArguments). Since we are using the `optimum-habana` library, we can use the [GaudiTrainer]() and [GaudiTrainingArguments]() instead. The `GaudiTrainer` is a wrapper around the [Trainer](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.Trainer that allows you to pre-traing or fine-tune a transformer model on a Habana Gaudi instances.\n",
    "\n",
    "```diff\n",
    "-from transformers import Trainer, TrainingArguments \n",
    "+from optimum.habana import GaudiTrainer, GaudiTrainingArguments\n",
    "\n",
    "# define the training arguments\n",
    "-training_args = TrainingArguments(\n",
    "+training_args = GaudiTrainingArguments(\n",
    "+  use_habana=True,\n",
    "+  use_lazy_mode=True,\n",
    "+  gaudi_config_name=path_to_gaudi_config,\n",
    "  ...\n",
    ")\n",
    "\n",
    "# Initialize our Trainer\n",
    "-trainer = Trainer(\n",
    "+trainer = GaudiTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset\n",
    "    ... # other arguments\n",
    ")\n",
    "```\n",
    "\n",
    "The `DL1` instance we use has 8 available HPU-cores meaning we can leverage distributed data-parallel training for our model. \n",
    "To run our training as distributed training we need to create a training script, which can be used with multiprocessing to run on all HPUs. \n",
    "We have created a [scripts/run_mlm.py](https://github.com/philschmid/deep-learning-habana-huggingface/blob/master/pre-training/scripts/run_mlm.py) implementing masked-language modeling using the `GaudiTrainer`. To executed our distributed training we use the `DistributedRunner` runner from `optimum-habana` and pass our arguments. Alternatively you could check-out the [gaudi_spawn.py](https://github.com/huggingface/optimum-habana/blob/main/examples/gaudi_spawn.py) in the [optimum-habana](https://github.com/huggingface/optimum-habana) repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "from optimum.habana.distributed import DistributedRunner\n",
    "\n",
    "\n",
    "max_steps=10_000\n",
    "world_size=8 # Number of HPUs to use (1 or 8)\n",
    "per_device_train_batch_size=16\n",
    "learning_rate=3e-5\n",
    "hf_hub_token=HfFolder.get_token()\n",
    "\n",
    "# define distributed runner\n",
    "distributed_runner = DistributedRunner(\n",
    "    command_list=[f\"scripts/run_mlm.py --dataset_id {dataset_id} --repository_id {tokenizer_id} --per_device_train_batch_size {per_device_train_batch_size} --learning_rate {learning_rate} --hf_hub_token {hf_hub_token}\"],\n",
    "    world_size=world_size,\n",
    "    use_mpi=True,\n",
    "    multi_hls=False,\n",
    ")\n",
    "\n",
    "# start job\n",
    "ret_code = distributed_runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "7a2c4b191d1ae843dde5cb5f4d1f62fa892f6b79b0f9392a84691e890e33c5a4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
