{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Training BERT with Hugging Face Transformers and Habana Gaudi\n",
    "\n",
    "In this tutorial, you will learn how to pre-train [BERT-base](https://huggingface.co/bert-base-uncased) from scratch using a Habana Gaudi-based [DL1 instance](https://aws.amazon.com/ec2/instance-types/dl1/) on AWS to take advantage of the cost performance benefits of Gaudi. We will use the Hugging Faces [Transformers](https://huggingface.co/docs/transformers), [Optimum Habana](https://huggingface.co/docs/optimum/main/en/habana_index) and [Datasets](https://huggingface.co/docs/datasets) library to pre-train a BERT-base model using masked-language modelling one of the two original BERT pre-training tasks. Before we get started, we need to set up the deep learning environment.\n",
    "\n",
    "You will learn how to:\n",
    "\n",
    "1. [Prepare the dataset](#1-prepare-the-dataset)\n",
    "2. [Train a Tokenizer](#2-train-a-tokenizer)\n",
    "3. [Preprocess the dataset](#3-preprocess-the-dataset)\n",
    "4. [Pre-train BERT on Habana Gaudi](#4-pre-train-bert-on-habana-gaudi)\n",
    "\n",
    "_Note: Step 1 to 3 can/should be run on a different instance size those are CPU intensive tasks._\n",
    "\n",
    "![pre-training overview](../assets/pre-training.png)\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "Before we can start make sure you have met the following requirements\n",
    "\n",
    "* AWS Account with quota for [DL1 instance type](https://aws.amazon.com/ec2/instance-types/dl1/)\n",
    "* [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) installed\n",
    "* AWS IAM user [configured in CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) with permission to create and manage ec2 instances\n",
    "\n",
    "**Helpful Resources**\n",
    "\n",
    "* [Setup Deep Learning environment for Hugging Face Transformers with Habana Gaudi on AWS](https://www.philschmid.de/getting-started-habana-gaudi)\n",
    "Gaudi on AWS](https://www.philschmid.de/getting-started-habana-gaudi)\n",
    "* [Deep Learning setup made easy with EC2 Remote Runner and Habana Gaudi](https://www.philschmid.de/habana-gaudi-ec2-runner)\n",
    "* [Optimum Habana Documentation](https://huggingface.co/docs/optimum/main/en/habana_index)\n",
    "* [Pre-training script](./scripts/run_mlm.py)\n",
    "\n",
    "\n",
    "\n",
    "## What is BERT? \n",
    "\n",
    "BERT, short for Bidirectional Encoder Representations from Transformers, is a Machine Learning (ML) model for natural language processing. It was developed in 2018 by researchers at Google AI Language and serves as a swiss army knife solution to 11+ of the most common language tasks, such as sentiment analysis and named entity recognition.\n",
    "\n",
    "Read more about BERT in our [BERT 101 ü§ó State Of The Art NLP Model Explained](https://huggingface.co/blog/bert-101) blog.\n",
    "\n",
    "## What is a Masked Language Modeling (MLM)?\n",
    "\n",
    "MLM enables/enforces bidirectional learning from text by masking (hiding) a word in a sentence and forcing BERT to bidirectionally use the words on either side of the covered word to predict the masked word.\n",
    "\n",
    "**Masked Language Modeling Example:**\n",
    "\n",
    "```bash\n",
    "‚ÄúDang! I‚Äôm out fishing and a huge trout just [MASK] my line!‚Äù\n",
    "```\n",
    "Read more about Masked Language Modeling [here](https://huggingface.co/blog/bert-101).\n",
    "\n",
    "--- \n",
    "\n",
    "Lets get started. üöÄ\n",
    "\n",
    "_Note: Step 1 to 3 where run on a AWS c6i.12xlarge instance._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare the dataset\n",
    "\n",
    "The Tutorial is \"split\" into two parts. The first part (step 1-3) is about preparing the dataset and tokenizer. The second part (step 4) is about pre-training BERT on the prepared dataset. Before we can start with the dataset preparation we need to setup our development environment. As mentioned in the introduction you don't need to prepare the dataset on the DL1 instance and could use your notebook or desktop computer. \n",
    "\n",
    "As first we are going to install `transformers`, `datsets` and `git-lfs` to push our Tokenizer and dataset to the [Hugging Face Hub](https://huggingface.co) for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets\n",
    "!sudo apt-get install git-lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to finish our setup lets log into the [Hugging Face Hub](https://huggingface.co/models) to push our dataset, tokenizer, model artifacts, logs and metrics during training and afterwards to the hub. \n",
    "\n",
    "_To be able to push our model to the Hub, you need to register on the [Hugging Face](https://huggingface.co/join)._\n",
    "\n",
    "We will use the `notebook_login` util from the `huggingface_hub` package to log into our account. You can get your token in the settings at [Access Tokens](https://huggingface.co/settings/tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0567f2ff4ae486bbf97cd09cdcb3ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are now logged in lets get the `user_id`, which will be used to push the artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user id 'philschmid' will be used during the example\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "user_id = HfApi().whoami()[\"name\"]\n",
    "\n",
    "print(f\"user id '{user_id}' will be used during the example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [original BERT](https://arxiv.org/abs/1810.04805) was pretrained on [Wikipedia](https://huggingface.co/datasets/wikipedia) and [BookCorpus](https://huggingface.co/datasets/bookcorpus) dataset. Both datasets are available on the [Hugging Face Hub](https://huggingface.co/datasets) and can be loaded with `datasets`. \n",
    "\n",
    "_Note: For wikipedia we will use the `20220301`, which is different to the original split._\n",
    "\n",
    "As a first step are we loading the dataset and merging them together to create on big dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset bookcorpus (/home/ubuntu/.cache/huggingface/datasets/bookcorpus/plain_text/1.0.0/44662c4a114441c35200992bea923b170e6f13f2f0beb7c14e43759cec498700)\n",
      "Reusing dataset wikipedia (/home/ubuntu/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559)\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets, load_dataset\n",
    "\n",
    "bookcorpus = load_dataset(\"bookcorpus\", split=\"train\")\n",
    "wiki = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\")\n",
    "wiki = wiki.remove_columns([col for col in wiki.column_names if col != \"text\"])  # only keep the 'text' column\n",
    "\n",
    "assert bookcorpus.features.type == wiki.features.type\n",
    "raw_datasets = concatenate_datasets([bookcorpus, wiki])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 80462898\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We are not going to do some advanced dataset preparation, like de-duplication, filtering or any other pre-processing. If you are planning to apply this notebook to train your own BERT model from scratch I highly recommend to including those data preparation steps into your workflow. This will help you improve your Language Model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train a Tokenizer\n",
    "\n",
    "To be able to train our model we need to convert our text into a tokenized format. Most Transformer models are coming with a pre-trained tokenizer, but since we are pre-training our model from scratch we also need to train a Tokenizer on our data. We can train a tokenizer on our data with `transformers` and the `BertTokenizerFast` class. \n",
    "\n",
    "More information about training a new tokenizer can be found in our [Hugging Face Course](https://huggingface.co/course/chapter6/2?fw=pt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# repositor id for saving the tokenizer\n",
    "tokenizer_id=\"bert-base-uncased-2022-habana\"\n",
    "\n",
    "# create a python generator to dynamically load the data\n",
    "def batch_iterator(batch_size=10000):\n",
    "    for i in tqdm(range(0, len(raw_datasets), batch_size)):\n",
    "        yield raw_datasets[i : i + batch_size][\"text\"]\n",
    "\n",
    "# create a tokenizer from existing one to re-use special tokens\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start training the tokenizer with `train_new_from_iterator()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8047/8047 [09:13<00:00, 14.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('tokenizer/tokenizer_config.json',\n",
       " 'tokenizer/special_tokens_map.json',\n",
       " 'tokenizer/vocab.txt',\n",
       " 'tokenizer/added_tokens.json',\n",
       " 'tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer = tokenizer.train_new_from_iterator(text_iterator=batch_iterator(), vocab_size=32_000)\n",
    "bert_tokenizer.save_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We push the tokenizer to [Hugging Face Hub](https://huggingface.co/models) for later training our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you need to be logged into push the tokenizer\n",
    "bert_tokenizer.push_to_hub(tokenizer_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocess the dataset\n",
    "\n",
    "The last step before we can get started with training our model is to pre-process/tokenize our dataset. We will use our trained tokenizer to tokenize our dataset and then push it to hub to load it easily later in our training. The tokenization process is also kept pretty simple, if documents are longer than `512` tokens those are truncated and not split into several documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max length for the tokenizer is: 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25a86c14dd8e44ef951383bfa75578b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80463 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (594 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'special_tokens_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import multiprocessing\n",
    "\n",
    "# load tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(f\"{user_id}/{tokenizer_id}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer\")\n",
    "num_proc = multiprocessing.cpu_count()\n",
    "print(f\"The max length for the tokenizer is: {tokenizer.model_max_length}\")\n",
    "\n",
    "def group_texts(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "       examples[\"text\"], return_special_tokens_mask=True, truncation=True, max_length=tokenizer.model_max_length\n",
    "    )\n",
    "    return tokenized_inputs\n",
    "\n",
    "# preprocess dataset\n",
    "tokenized_datasets = raw_datasets.map(group_texts, batched=True, remove_columns=[\"text\"], num_proc=num_proc)\n",
    "tokenized_datasets.features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As data processing function will we concatenate all texts from our dataset and generate chunks of `tokenizer.model_max_length` (512)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
    "# max_seq_length.\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= tokenizer.model_max_length:\n",
    "        total_length = (total_length // tokenizer.model_max_length) * tokenizer.model_max_length\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + tokenizer.model_max_length] for i in range(0, total_length, tokenizer.model_max_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.map(group_texts, batched=True, num_proc=num_proc)\n",
    "# shuffle dataset\n",
    "tokenized_datasets = tokenized_datasets.shuffle(seed=34)\n",
    "\n",
    "print(f\"the dataset contains in total {len(tokenized_datasets)*tokenizer.model_max_length} tokens\")\n",
    "# the dataset contains in total 3417216000 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step before we can start with out training is to push our prepared dataset to the hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03cfdd3a9d4843a18f79482cec8bed8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.9/site-packages/huggingface_hub/hf_api.py:1948: FutureWarning: `identical_ok` has no effect and is deprecated. It will be removed in 0.11.0.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# push dataset to hugging face\n",
    "dataset_id=f\"{user_id}/processed_bert_dataset\"\n",
    "tokenized_datasets.push_to_hub(f\"{user_id}/processed_bert_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pre-train BERT on Habana Gaudi\n",
    "\n",
    "In this example are we going to use Habana Gaudi on AWS using the DL1 instance for running the pre-training. We will use the [Remote Runner](https://github.com/philschmid/deep-learning-remote-runner) toolkit to easily launch our pre-training on a remote DL1 Instance from our local setup. You can check-out [Deep Learning setup made easy with EC2 Remote Runner and Habana Gaudi](https://www.philschmid.de/habana-gaudi-ec2-runner) if you want to know more about how this works. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rm-runner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using GPUs you would use the [Trainer](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.Trainer) and [TrainingArguments](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.TrainingArguments). Since we are going to run our training on Habana Gaudi we are leveraging the `optimum-habana` library, we can use the [GaudiTrainer](https://huggingface.co/docs/optimum/main/en/habana_trainer) and GaudiTrainingArguments instead. The `GaudiTrainer` is a wrapper around the [Trainer](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.Trainer that allows you to pre-traing or fine-tune a transformer model on a Habana Gaudi instances.\n",
    "\n",
    "```diff\n",
    "-from transformers import Trainer, TrainingArguments \n",
    "+from optimum.habana import GaudiTrainer, GaudiTrainingArguments\n",
    "\n",
    "# define the training arguments\n",
    "-training_args = TrainingArguments(\n",
    "+training_args = GaudiTrainingArguments(\n",
    "+  use_habana=True,\n",
    "+  use_lazy_mode=True,\n",
    "+  gaudi_config_name=path_to_gaudi_config,\n",
    "  ...\n",
    ")\n",
    "\n",
    "# Initialize our Trainer\n",
    "-trainer = Trainer(\n",
    "+trainer = GaudiTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset\n",
    "    ... # other arguments\n",
    ")\n",
    "```\n",
    "\n",
    "The `DL1` instance we use has 8 available HPU-cores meaning we can leverage distributed data-parallel training for our model. \n",
    "To run our training as distributed training we need to create a training script, which can be used with multiprocessing to run on all HPUs. \n",
    "We have created a [scripts/run_mlm.py](https://github.com/philschmid/deep-learning-habana-huggingface/blob/master/pre-training/scripts/run_mlm.py) implementing masked-language modeling using the `GaudiTrainer`. To executed our distributed training we use the `DistributedRunner` runner from `optimum-habana` and pass our arguments. Alternatively you could check-out the [gaudi_spawn.py](https://github.com/huggingface/optimum-habana/blob/main/examples/gaudi_spawn.py) in the [optimum-habana](https://github.com/huggingface/optimum-habana) repository.\n",
    "\n",
    "\n",
    "Before we can start our training we need to define the `hyperparameters` we want to use for our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# hyperparameters\n",
    "hyperparameters = {\n",
    "    \"model_config_id\": \"bert-base-uncased\",\n",
    "    \"dataset_id\": \"philschmid/processed_bert_dataset\",\n",
    "    \"tokenizer_id\": \"philschmid/bert-base-uncased-2022-habana\",\n",
    "    \"gaudi_config_id\": \"philschmid/bert-base-uncased-2022-habana\",\n",
    "    \"repository_id\": \"bert-base-uncased-2022-habana-test-6\",\n",
    "    \"hf_hub_token\": HfFolder.get_token(),  # need to be login in with `huggingface-cli login`\n",
    "    \"max_steps\": 70_000,\n",
    "    \"per_device_train_batch_size\": 32,\n",
    "    \"learning_rate\": 5e-5,\n",
    "}\n",
    "hyperparameters_string = \" \".join(f\"--{key} {value}\" for key, value in hyperparameters.items())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start our training with by creating a `EC2RemoteRunner` and then `launch` it. This will then start our AWS EC2 DL1 instance and runs our `run_mlm.py` script on it using the `huggingface/optimum-habana:latest` container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-18 08:56:39,681 | INFO | Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2022-08-18 08:56:40,872 | INFO | Created key pair: rm-runner-fhbi\n",
      "2022-08-18 08:56:42,018 | INFO | Created security group: rm-runner-fhbi\n",
      "2022-08-18 08:56:43,564 | INFO | Launched instance: i-0e238bc3f7edaeaf1\n",
      "2022-08-18 08:56:43,568 | INFO | Waiting for instance to be ready...\n",
      "2022-08-18 08:56:59,652 | INFO | Instance is ready. Public DNS: ec2-44-202-19-228.compute-1.amazonaws.com\n",
      "2022-08-18 08:56:59,683 | INFO | Setting up ssh connection...\n",
      "2022-08-18 08:58:19,704 | INFO | Setting up ssh connection...\n",
      "2022-08-18 08:58:54,930 | INFO | Connected (version 2.0, client OpenSSH_8.2p1)\n",
      "2022-08-18 08:58:56,289 | INFO | Authentication (publickey) successful!\n",
      "2022-08-18 08:58:56,293 | INFO | Pulling container: huggingface/optimum-habana:4.21.1-pt1.11.0-synapse1.5.0...\n",
      "2022-08-18 09:00:15,641 | INFO | Uploading from scripts\n",
      "2022-08-18 09:00:16,767 | INFO | Executing: docker run --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --net=host --ipc=host -v /home/ubuntu/test:/home/ubuntu/rm-runner --workdir=/home/ubuntu/rm-runner huggingface/optimum-habana:4.21.1-pt1.11.0-synapse1.5.0 python3 gaudi_spawn.py --use_mpi --world_size=8 run_mlm.py --model_config_id bert-base-uncased --dataset_id philschmid/processed_bert_dataset --tokenizer_id philschmid/bert-base-uncased-2022-habana --gaudi_config_id philschmid/bert-base-uncased-2022-habana --repository_id bert-base-uncased-2022-habana-test-6 --hf_hub_token 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 --max_steps 70000 --per_device_train_batch_size 32 --learning_rate 5e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with the following model specific env vars: \n",
      "MASTER_ADDR=localhost\n",
      "MASTER_PORT=12345\n",
      "DistributedRunner run(): command = mpirun -n 8 --bind-to core --map-by socket:PE=6 --rank-by core --report-bindings --allow-run-as-root /usr/bin/python3 run_mlm.py --model_config_id bert-base-uncased --dataset_id philschmid/processed_bert_dataset --tokenizer_id philschmid/bert-base-uncased-2022-habana --gaudi_config_id philschmid/bert-base-uncased-2022-habana --repository_id bert-base-uncased-2022-habana-test-6 --hf_hub_token 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 --max_steps 70000 --per_device_train_batch_size 32 --learning_rate 5e-05\n",
      "[ip-172-31-93-36:00189] MCW rank 2 bound to socket 0[core 12[hwt 0-1]], socket 0[core 13[hwt 0-1]], socket 0[core 14[hwt 0-1]], socket 0[core 15[hwt 0-1]], socket 0[core 16[hwt 0-1]], socket 0[core 17[hwt 0-1]]: [../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/../../../../../..][../../../../../../../../../../../../../../../../../../../../../../../..]\n",
      "[ip-172-31-93-36:00189] MCW rank 3 bound to socket 0[core 18[hwt 0-1]], socket 0[core 19[hwt 0-1]], socket 0[core 20[hwt 0-1]], socket 0[core 21[hwt 0-1]], socket 0[core 22[hwt 0-1]], socket 0[core 23[hwt 0-1]]: [../../../../../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB][../../../../../../../../../../../../../../../../../../../../../../../..]\n",
      "[ip-172-31-93-36:00189] MCW rank 4 bound to socket 1[core 24[hwt 0-1]], socket 1[core 25[hwt 0-1]], socket 1[core 26[hwt 0-1]], socket 1[core 27[hwt 0-1]], socket 1[core 28[hwt 0-1]], socket 1[core 29[hwt 0-1]]: [../../../../../../../../../../../../../../../../../../../../../../../..][BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../../../../../..]\n",
      "[ip-172-31-93-36:00189] MCW rank 5 bound to socket 1[core 30[hwt 0-1]], socket 1[core 31[hwt 0-1]], socket 1[core 32[hwt 0-1]], socket 1[core 33[hwt 0-1]], socket 1[core 34[hwt 0-1]], socket 1[core 35[hwt 0-1]]: [../../../../../../../../../../../../../../../../../../../../../../../..][../../../../../../BB/BB/BB/BB/BB/BB/../../../../../../../../../../../..]\n",
      "[ip-172-31-93-36:00189] MCW rank 6 bound to socket 1[core 36[hwt 0-1]], socket 1[core 37[hwt 0-1]], socket 1[core 38[hwt 0-1]], socket 1[core 39[hwt 0-1]], socket 1[core 40[hwt 0-1]], socket 1[core 41[hwt 0-1]]: [../../../../../../../../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/../../../../../..]\n",
      "[ip-172-31-93-36:00189] MCW rank 7 bound to socket 1[core 42[hwt 0-1]], socket 1[core 43[hwt 0-1]], socket 1[core 44[hwt 0-1]], socket 1[core 45[hwt 0-1]], socket 1[core 46[hwt 0-1]], socket 1[core 47[hwt 0-1]]: [../../../../../../../../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB]\n",
      "[ip-172-31-93-36:00189] MCW rank 0 bound to socket 0[core 0[hwt 0-1]], socket 0[core 1[hwt 0-1]], socket 0[core 2[hwt 0-1]], socket 0[core 3[hwt 0-1]], socket 0[core 4[hwt 0-1]], socket 0[core 5[hwt 0-1]]: [BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../../../../../../../..]\n",
      "[ip-172-31-93-36:00189] MCW rank 1 bound to socket 0[core 6[hwt 0-1]], socket 0[core 7[hwt 0-1]], socket 0[core 8[hwt 0-1]], socket 0[core 9[hwt 0-1]], socket 0[core 10[hwt 0-1]], socket 0[core 11[hwt 0-1]]: [../../../../../../BB/BB/BB/BB/BB/BB/../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../../../../../../../..]\n",
      "Collecting git+https://github.com/huggingface/optimum-habana.git\n",
      "  Cloning https://github.com/huggingface/optimum-habana.git to /tmp/pip-req-build-pkcd07vp\n",
      "Collecting git+https://github.com/huggingface/optimum-habana.git\n",
      "Collecting git+https://github.com/huggingface/optimum-habana.git\n",
      "Collecting git+https://github.com/huggingface/optimum-habana.git\n",
      "  Running command git clone -q https://github.com/huggingface/optimum-habana.git /tmp/pip-req-build-pkcd07vp\n",
      "  Cloning https://github.com/huggingface/optimum-habana.git to /tmp/pip-req-build-zagp3vji\n",
      "  Cloning https://github.com/huggingface/optimum-habana.git to /tmp/pip-req-build-po96eyyv\n",
      "  Cloning https://github.com/huggingface/optimum-habana.git to /tmp/pip-req-build-wcmfxmqd\n",
      "  Running command git clone -q https://github.com/huggingface/optimum-habana.git /tmp/pip-req-build-zagp3vji\n",
      "  Running command git clone -q https://github.com/huggingface/optimum-habana.git /tmp/pip-req-build-po96eyyv\n",
      "  Running command git clone -q https://github.com/huggingface/optimum-habana.git /tmp/pip-req-build-wcmfxmqd\n",
      "Collecting git+https://github.com/huggingface/optimum-habana.git\n",
      "Collecting git+https://github.com/huggingface/optimum-habana.git\n",
      "Collecting git+https://github.com/huggingface/optimum-habana.git\n",
      "  Cloning https://github.com/huggingface/optimum-habana.git to /tmp/pip-req-build-z23egyss\n",
      "  Cloning https://github.com/huggingface/optimum-habana.git to /tmp/pip-req-build-1ow6fxkt\n",
      "  Cloning https://github.com/huggingface/optimum-habana.git to /tmp/pip-req-build-ldwjbjf7\n",
      "  Running command git clone -q https://github.com/huggingface/optimum-habana.git /tmp/pip-req-build-z23egyss\n",
      "  Running command git clone -q https://github.com/huggingface/optimum-habana.git /tmp/pip-req-build-1ow6fxkt\n",
      "  Running command git clone -q https://github.com/huggingface/optimum-habana.git /tmp/pip-req-build-ldwjbjf7\n",
      "Collecting git+https://github.com/huggingface/optimum-habana.git\n",
      "  Cloning https://github.com/huggingface/optimum-habana.git to /tmp/pip-req-build-uw_oclx5\n",
      "  Running command git clone -q https://github.com/huggingface/optimum-habana.git /tmp/pip-req-build-uw_oclx5\n",
      "  Installing build dependencies ... \u001b[?25l-  Installing build dependencies ... \u001b[?25l-  Installing build dependencies ... \u001b[?25l-  Installing build dependencies ... \u001b[?25l-  Installing build dependencies ... \u001b[?25l-  Installing build dependencies ... \u001b[?25l-  Installing build dependencies ... \u001b[?25l-  Installing build dependencies ... \u001b[?25ldone\n",
      "\bdone\n",
      "\bdone\n",
      "\bdone\n",
      "\bdone\n",
      "\bdone\n",
      "\bdone\n",
      "\bdone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\bdone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\bdone\n",
      "\bdone\n",
      "\bdone\n",
      "\bdone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25l-\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (2.4.0)\n",
      "\u001b[?25hRequirement already satisfied: tokenizers in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (0.12.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (2.4.0)\n",
      "\bdone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25l-\u001b[?25h    Preparing wheel metadata ... \u001b[?25l-\u001b[?25h    Preparing wheel metadata ... \u001b[?25l-\u001b[?25h    Preparing wheel metadata ... \u001b[?25l-\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (0.1.97)\n",
      "Requirement already satisfied: tokenizers in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (0.12.1)\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25l-Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (1.11.0a0+gita4c10ee)\n",
      "Requirement already satisfied: transformers>=4.20.0 in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (4.21.1)\n",
      "\bdone\n",
      "\bdone\n",
      "\bdone\n",
      "\u001b[?25hRequirement already satisfied: optimum in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (1.3.0)\n",
      "\u001b[?25hRequirement already satisfied: tokenizers in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (0.12.1)\n",
      "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (0.1.97)\n",
      "Requirement already satisfied: optimum in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (1.3.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (0.1.97)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (1.11.0a0+gita4c10ee)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (1.9.0)\n",
      "Requirement already satisfied: optimum in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (1.3.0)\n",
      "Requirement already satisfied: optimum in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (1.3.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (0.1.97)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (1.9.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (9.2.0)\n",
      "Requirement already satisfied: transformers>=4.20.0 in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (4.21.1)\n",
      "Requirement already satisfied: tokenizers in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (0.12.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (1.11.0a0+gita4c10ee)\n",
      "Requirement already satisfied: transformers>=4.20.0 in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (4.21.1)\n",
      "Requirement already satisfied: tokenizers in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (0.12.1)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (9.2.0)\n",
      "\bdone\n",
      "Requirement already satisfied: transformers>=4.20.0 in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (4.21.1)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (9.2.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (1.9.0)\n",
      "\u001b[?25hRequirement already satisfied: optimum in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (1.3.0)\n",
      "Requirement already satisfied: transformers>=4.20.0 in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (4.21.1)\n",
      "Requirement already satisfied: transformers>=4.20.0 in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (4.21.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (2.4.0)\n",
      "\bdone\n",
      "\u001b[?25hRequirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (2.4.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (0.1.97)\n",
      "Requirement already satisfied: transformers>=4.20.0 in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (4.21.1)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (9.2.0)\n",
      "Requirement already satisfied: optimum in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (1.3.0)\n",
      "Requirement already satisfied: tokenizers in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (0.12.1)\n",
      "Requirement already satisfied: transformers>=4.20.0 in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (4.21.1)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (9.2.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (2.4.0)\n",
      "Requirement already satisfied: optimum in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (1.3.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (1.9.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (1.11.0a0+gita4c10ee)\n",
      "Requirement already satisfied: tokenizers in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (0.12.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (2.4.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (9.2.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (2.4.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (2.4.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (1.11.0a0+gita4c10ee)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (2.28.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (0.1.97)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (1.9.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (4.64.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (2022.5.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (21.3)\n",
      "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (0.3.5.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (0.70.13)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (2022.5.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (0.8.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (1.9.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (9.0.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->optimum-habana==1.2.0.dev0) (4.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers>=4.20.0->optimum-habana==1.2.0.dev0) (3.8.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (1.22.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (3.0.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (1.4.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.20.0->optimum-habana==1.2.0.dev0) (0.8.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (0.70.13)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (3.8.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (3.8.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (21.3)\n",
      "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (0.3.5.1)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (0.18.0)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (0.18.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.20.0->optimum-habana==1.2.0.dev0) (4.64.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (2.28.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (9.2.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (1.22.3)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (9.0.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (4.64.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (0.8.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.20.0->optimum-habana==1.2.0.dev0) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.20.0->optimum-habana==1.2.0.dev0) (2020.10.28)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers>=4.20.0->optimum-habana==1.2.0.dev0) (2.28.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->optimum-habana==1.2.0.dev0) (4.2.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (1.4.1)\n",
      "Requirement already satisfied: numpy<1.25.0,>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from scipy->optimum-habana==1.2.0.dev0) (1.22.3)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from optimum->optimum-habana==1.2.0.dev0) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.20.0->optimum-habana==1.2.0.dev0) (1.22.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from optimum->optimum-habana==1.2.0.dev0) (0.8.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.20.0->optimum-habana==1.2.0.dev0) (21.3)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.8/dist-packages (from optimum->optimum-habana==1.2.0.dev0) (15.0.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (0.1.97)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from optimum->optimum-habana==1.2.0.dev0) (1.10.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from optimum->optimum-habana==1.2.0.dev0) (1.10.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->optimum-habana==1.2.0.dev0) (4.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.20.0->optimum-habana==1.2.0.dev0) (2020.10.28)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.8/dist-packages (from optimum->optimum-habana==1.2.0.dev0) (15.0.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (9.0.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (2022.5.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers>=4.20.0->optimum-habana==1.2.0.dev0) (3.8.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (1.11.0a0+gita4c10ee)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from optimum->optimum-habana==1.2.0.dev0) (21.3)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.8/dist-packages (from optimum->optimum-habana==1.2.0.dev0) (15.0.1)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.8/dist-packages (from optimum->optimum-habana==1.2.0.dev0) (15.0.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from optimum->optimum-habana==1.2.0.dev0) (1.22.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from optimum->optimum-habana==1.2.0.dev0) (0.8.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from optimum->optimum-habana==1.2.0.dev0) (1.10.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from optimum->optimum-habana==1.2.0.dev0) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.20.0->optimum-habana==1.2.0.dev0) (5.4.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.20.0->optimum-habana==1.2.0.dev0) (5.4.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from optimum->optimum-habana==1.2.0.dev0) (1.10.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from optimum->optimum-habana==1.2.0.dev0) (1.22.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.20.0->optimum-habana==1.2.0.dev0) (2020.10.28)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from optimum->optimum-habana==1.2.0.dev0) (1.10.1)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.8/dist-packages (from optimum->optimum-habana==1.2.0.dev0) (15.0.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers>=4.20.0->optimum-habana==1.2.0.dev0) (3.8.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from optimum->optimum-habana==1.2.0.dev0) (0.8.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets->optimum-habana==1.2.0.dev0) (2022.5.18.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets->optimum-habana==1.2.0.dev0) (1.26.9)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->optimum-habana==1.2.0.dev0) (4.2.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (9.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets->optimum-habana==1.2.0.dev0) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum-habana==1.2.0.dev0) (2.8.2)\n",
      "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (0.3.5.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum-habana==1.2.0.dev0) (2022.1)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.8/dist-packages (from optimum->optimum-habana==1.2.0.dev0) (15.0.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (1.4.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (2.0.12)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from optimum->optimum-habana==1.2.0.dev0) (21.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (1.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets->optimum-habana==1.2.0.dev0) (3.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.20.0->optimum-habana==1.2.0.dev0) (2020.10.28)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets->optimum-habana==1.2.0.dev0) (2.0.12)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers>=4.20.0->optimum-habana==1.2.0.dev0) (3.8.0)\n",
      "Requirement already satisfied: tokenizers in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (0.12.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers>=4.20.0->optimum-habana==1.2.0.dev0) (3.8.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (1.7.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (21.4.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (3.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.20.0->optimum-habana==1.2.0.dev0) (5.4.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (1.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers>=4.20.0->optimum-habana==1.2.0.dev0) (2.28.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (0.70.13)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.20.0->optimum-habana==1.2.0.dev0) (4.64.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (21.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.20.0->optimum-habana==1.2.0.dev0) (5.4.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (3.8.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers>=4.20.0->optimum-habana==1.2.0.dev0) (2.28.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.20.0->optimum-habana==1.2.0.dev0) (2020.10.28)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.20.0->optimum-habana==1.2.0.dev0) (4.64.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers>=4.20.0->optimum-habana==1.2.0.dev0) (3.8.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers>=4.20.0->optimum-habana==1.2.0.dev0) (2.28.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (1.11.0a0+gita4c10ee)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (0.18.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (1.9.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from optimum->optimum-habana==1.2.0.dev0) (1.22.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.20.0->optimum-habana==1.2.0.dev0) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.20.0->optimum-habana==1.2.0.dev0) (4.64.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from optimum->optimum-habana==1.2.0.dev0) (1.10.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.20.0->optimum-habana==1.2.0.dev0) (2020.10.28)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->optimum-habana==1.2.0.dev0) (4.2.0)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.8/dist-packages (from optimum->optimum-habana==1.2.0.dev0) (15.0.1)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (0.18.0)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (0.18.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from optimum->optimum-habana==1.2.0.dev0) (21.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.20.0->optimum-habana==1.2.0.dev0) (2022.5.18.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.20.0->optimum-habana==1.2.0.dev0) (2.0.12)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from optimum->optimum-habana==1.2.0.dev0) (0.8.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (1.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (1.7.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.20.0->optimum-habana==1.2.0.dev0) (1.26.9)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (1.2.0)\n",
      "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (0.3.5.1)\n",
      "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (0.3.5.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (4.0.2)\n",
      "Requirement already satisfied: urllib3>=1.25.10 in /usr/local/lib/python3.8/dist-packages (from responses<0.19->datasets->optimum-habana==1.2.0.dev0) (1.26.9)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (0.70.13)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (0.70.13)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (3.0.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets->optimum-habana==1.2.0.dev0) (3.0.9)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (3.0.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (9.0.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (1.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (0.70.13)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum-habana==1.2.0.dev0) (2022.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (1.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum-habana==1.2.0.dev0) (2.8.2)\n",
      "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (0.3.5.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->optimum->optimum-habana==1.2.0.dev0) (1.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.20.0->optimum-habana==1.2.0.dev0) (3.3)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (9.0.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers>=4.20.0->optimum-habana==1.2.0.dev0) (3.0.9)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (0.18.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets->optimum-habana==1.2.0.dev0) (3.3)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.8/dist-packages (from coloredlogs->optimum->optimum-habana==1.2.0.dev0) (10.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.8/dist-packages (from coloredlogs->optimum->optimum-habana==1.2.0.dev0) (10.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (9.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets->optimum-habana==1.2.0.dev0) (2022.5.18.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (3.8.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (2022.5.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.8/dist-packages (from coloredlogs->optimum->optimum-habana==1.2.0.dev0) (10.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (2.28.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->optimum->optimum-habana==1.2.0.dev0) (1.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum-habana==1.2.0.dev0) (1.16.0)\n",
      "Building wheels for collected packages: optimum-habana\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->optimum->optimum-habana==1.2.0.dev0) (1.2.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (2022.5.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum-habana==1.2.0.dev0) (2022.1)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (0.18.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum-habana==1.2.0.dev0) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum-habana==1.2.0.dev0) (1.16.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (2022.5.0)\n",
      "Building wheels for collected packages: optimum-habana\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (21.4.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (0.1.97)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (4.64.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (1.9.0)\n",
      "Requirement already satisfied: optimum in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (1.3.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (3.8.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (9.0.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (3.0.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (3.8.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (3.8.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->optimum->optimum-habana==1.2.0.dev0) (1.2.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->optimum->optimum-habana==1.2.0.dev0) (1.2.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (1.3.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.8/dist-packages (from coloredlogs->optimum->optimum-habana==1.2.0.dev0) (10.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.8/dist-packages (from coloredlogs->optimum->optimum-habana==1.2.0.dev0) (10.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->optimum-habana==1.2.0.dev0) (4.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (1.7.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.8/dist-packages (from coloredlogs->optimum->optimum-habana==1.2.0.dev0) (10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->optimum->optimum-habana==1.2.0.dev0) (3.0.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->optimum->optimum-habana==1.2.0.dev0) (3.0.9)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (4.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->optimum->optimum-habana==1.2.0.dev0) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.20.0->optimum-habana==1.2.0.dev0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.20.0->optimum-habana==1.2.0.dev0) (1.26.9)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (1.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.20.0->optimum-habana==1.2.0.dev0) (2022.5.18.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->optimum->optimum-habana==1.2.0.dev0) (1.2.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (2022.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum-habana==1.2.0.dev0) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.20.0->optimum-habana==1.2.0.dev0) (1.26.9)\n",
      "Building wheels for collected packages: optimum-habana\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.20.0->optimum-habana==1.2.0.dev0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.20.0->optimum-habana==1.2.0.dev0) (1.26.9)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from optimum-habana==1.2.0.dev0) (1.11.0a0+gita4c10ee)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (1.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.20.0->optimum-habana==1.2.0.dev0) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.20.0->optimum-habana==1.2.0.dev0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.20.0->optimum-habana==1.2.0.dev0) (2.0.12)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.20.0->optimum-habana==1.2.0.dev0) (2.0.12)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum-habana==1.2.0.dev0) (2022.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum-habana==1.2.0.dev0) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum-habana==1.2.0.dev0) (2.8.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum-habana==1.2.0.dev0) (2.8.2)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (0.70.13)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (21.4.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.20.0->optimum-habana==1.2.0.dev0) (2.0.12)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (3.0.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (3.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.20.0->optimum-habana==1.2.0.dev0) (2022.5.18.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (1.7.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (21.3)\n",
      "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (0.3.5.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum-habana==1.2.0.dev0) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum-habana==1.2.0.dev0) (2.8.2)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (0.70.13)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (1.22.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (1.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (2.28.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (1.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers>=4.20.0->optimum-habana==1.2.0.dev0) (3.8.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (21.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (1.7.2)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (9.0.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (1.3.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (2022.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.20.0->optimum-habana==1.2.0.dev0) (5.4.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (21.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.20.0->optimum-habana==1.2.0.dev0) (2020.10.28)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->optimum-habana==1.2.0.dev0) (4.2.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->optimum->optimum-habana==1.2.0.dev0) (1.2.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.8/dist-packages (from coloredlogs->optimum->optimum-habana==1.2.0.dev0) (10.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (1.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->optimum->optimum-habana==1.2.0.dev0) (3.0.9)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (4.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets->optimum-habana==1.2.0.dev0) (2022.5.18.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (1.7.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets->optimum-habana==1.2.0.dev0) (2.0.12)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (1.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets->optimum-habana==1.2.0.dev0) (3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (4.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum-habana==1.2.0.dev0) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets->optimum-habana==1.2.0.dev0) (1.26.9)\n",
      "Building wheels for collected packages: optimum-habana\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum-habana==1.2.0.dev0) (1.16.0)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (0.18.0)\n",
      "Building wheels for collected packages: optimum-habana\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum-habana==1.2.0.dev0) (1.16.0)\n",
      "Building wheels for collected packages: optimum-habana\n",
      "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (0.3.5.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (4.64.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (21.4.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (0.8.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (4.0.2)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-habana==1.2.0.dev0) (3.8.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (1.7.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (1.3.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum-habana==1.2.0.dev0) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum-habana==1.2.0.dev0) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum-habana==1.2.0.dev0) (1.16.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers>=4.20.0->optimum-habana==1.2.0.dev0) (3.8.0)\n",
      "Building wheels for collected packages: optimum-habana\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.20.0->optimum-habana==1.2.0.dev0) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.20.0->optimum-habana==1.2.0.dev0) (2020.10.28)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.8/dist-packages (from optimum->optimum-habana==1.2.0.dev0) (15.0.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from optimum->optimum-habana==1.2.0.dev0) (1.10.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->optimum-habana==1.2.0.dev0) (4.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum-habana==1.2.0.dev0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum-habana==1.2.0.dev0) (2022.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets->optimum-habana==1.2.0.dev0) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets->optimum-habana==1.2.0.dev0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets->optimum-habana==1.2.0.dev0) (2022.5.18.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets->optimum-habana==1.2.0.dev0) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets->optimum-habana==1.2.0.dev0) (2.0.12)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (21.4.0)\n",
      "  Building wheel for optimum-habana (PEP 517) ... \u001b[?25l-Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-habana==1.2.0.dev0) (1.7.2)\n",
      "  Building wheel for optimum-habana (PEP 517) ... \u001b[?25l-Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.8/dist-packages (from coloredlogs->optimum->optimum-habana==1.2.0.dev0) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->optimum->optimum-habana==1.2.0.dev0) (1.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum-habana==1.2.0.dev0) (1.16.0)\n",
      "Building wheels for collected packages: optimum-habana\n",
      "  Building wheel for optimum-habana (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for optimum-habana: filename=optimum_habana-1.2.0.dev0-cp38-none-any.whl size=62510 sha256=94f02227a848afe689db72ab62d54ac41a6c07174e663304ba7e7ebd34c9712e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-pm4jpdjk/wheels/e7/d1/3d/c30f79d8625c75bfa12ddbd0b71c575a61eba5b6707a4f6873\n",
      "\bdone\n",
      "\u001b[?25h  Created wheel for optimum-habana: filename=optimum_habana-1.2.0.dev0-cp38-none-any.whl size=62510 sha256=94f02227a848afe689db72ab62d54ac41a6c07174e663304ba7e7ebd34c9712e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-1ujih98_/wheels/e7/d1/3d/c30f79d8625c75bfa12ddbd0b71c575a61eba5b6707a4f6873\n",
      "  Building wheel for optimum-habana (PEP 517) ... \u001b[?25l-  Building wheel for optimum-habana (PEP 517) ... \u001b[?25l-Successfully built optimum-habana\n",
      "  Building wheel for optimum-habana (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for optimum-habana: filename=optimum_habana-1.2.0.dev0-cp38-none-any.whl size=62510 sha256=94f02227a848afe689db72ab62d54ac41a6c07174e663304ba7e7ebd34c9712e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-44k7ugpq/wheels/e7/d1/3d/c30f79d8625c75bfa12ddbd0b71c575a61eba5b6707a4f6873\n",
      "Successfully built optimum-habana\n",
      "  Building wheel for optimum-habana (PEP 517) ... \u001b[?25l-Successfully built optimum-habana\n",
      "\bdone\n",
      "\bdone\n",
      "\u001b[?25h  Created wheel for optimum-habana: filename=optimum_habana-1.2.0.dev0-cp38-none-any.whl size=62510 sha256=94f02227a848afe689db72ab62d54ac41a6c07174e663304ba7e7ebd34c9712e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-zs6m4mc1/wheels/e7/d1/3d/c30f79d8625c75bfa12ddbd0b71c575a61eba5b6707a4f6873\n",
      "\u001b[?25h  Created wheel for optimum-habana: filename=optimum_habana-1.2.0.dev0-cp38-none-any.whl size=62510 sha256=94f02227a848afe689db72ab62d54ac41a6c07174e663304ba7e7ebd34c9712e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-chqn_sz2/wheels/e7/d1/3d/c30f79d8625c75bfa12ddbd0b71c575a61eba5b6707a4f6873\n",
      "\bdone\n",
      "\u001b[?25h  Created wheel for optimum-habana: filename=optimum_habana-1.2.0.dev0-cp38-none-any.whl size=62510 sha256=94f02227a848afe689db72ab62d54ac41a6c07174e663304ba7e7ebd34c9712e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-gh9ouodr/wheels/e7/d1/3d/c30f79d8625c75bfa12ddbd0b71c575a61eba5b6707a4f6873\n",
      "  Building wheel for optimum-habana (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for optimum-habana: filename=optimum_habana-1.2.0.dev0-cp38-none-any.whl size=62510 sha256=94f02227a848afe689db72ab62d54ac41a6c07174e663304ba7e7ebd34c9712e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-4bhx7y2_/wheels/e7/d1/3d/c30f79d8625c75bfa12ddbd0b71c575a61eba5b6707a4f6873\n",
      "Successfully built optimum-habana\n",
      "Successfully built optimum-habana\n",
      "Successfully built optimum-habana\n",
      "Successfully built optimum-habana\n",
      "\bdone\n",
      "\u001b[?25h  Created wheel for optimum-habana: filename=optimum_habana-1.2.0.dev0-cp38-none-any.whl size=62510 sha256=94f02227a848afe689db72ab62d54ac41a6c07174e663304ba7e7ebd34c9712e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-p3bue6v4/wheels/e7/d1/3d/c30f79d8625c75bfa12ddbd0b71c575a61eba5b6707a4f6873\n",
      "Successfully built optimum-habana\n",
      "Installing collected packages: optimum-habana\n",
      "  Found existing installation: optimum-habana 1.1.1\n",
      "    Uninstalling optimum-habana-1.1.1:\n",
      "      Successfully uninstalled optimum-habana-1.1.1\n",
      "Installing collected packages: optimum-habana\n",
      "  Found existing installation: optimum-habana 1.1.1\n",
      "    Can't uninstall 'optimum-habana'. No files were found to uninstall.\n",
      "ERROR: Error checking for conflicts.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3021, in _dep_map\n",
      "    return self.__dep_map\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _DistInfoDistribution__dep_map\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3012, in _parsed_pkg_info\n",
      "    return self._pkg_info\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _pkg_info\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/commands/install.py\", line 568, in _warn_about_conflicts\n",
      "    package_set, _dep_info = check_install_conflicts(to_install)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/operations/check.py\", line 114, in check_install_conflicts\n",
      "    package_set, _ = create_package_set_from_installed()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/operations/check.py\", line 53, in create_package_set_from_installed\n",
      "    package_set[name] = PackageDetails(dist.version, dist.requires())\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2736, in requires\n",
      "    dm = self._dep_map\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3023, in _dep_map\n",
      "    self.__dep_map = self._compute_dependencies()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3032, in _compute_dependencies\n",
      "    for req in self._parsed_pkg_info.get_all('Requires-Dist') or []:\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3014, in _parsed_pkg_info\n",
      "    metadata = self.get_metadata(self.PKG_INFO)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 1420, in get_metadata\n",
      "    value = self._get(path)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 1616, in _get\n",
      "    with open(path, 'rb') as stream:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/lib/python3.8/dist-packages/optimum_habana-1.1.1.dist-info/METADATA'\n",
      "Installing collected packages: optimum-habana\n",
      "  Found existing installation: optimum-habana 1.1.1\n",
      "    Can't uninstall 'optimum-habana'. No files were found to uninstall.\n",
      "Installing collected packages: optimum-habana\n",
      "  Found existing installation: optimum-habana 1.1.1\n",
      "    Can't uninstall 'optimum-habana'. No files were found to uninstall.\n",
      "Successfully installed optimum-habana-1.2.0.dev0\n",
      "Successfully installed optimum-habana-1.2.0.dev0\n",
      "Successfully installed optimum-habana-1.2.0.dev0\n",
      "Installing collected packages: optimum-habana\n",
      "  Found existing installation: optimum-habana 1.1.1\n",
      "ERROR: Error checking for conflicts.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3021, in _dep_map\n",
      "    return self.__dep_map\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _DistInfoDistribution__dep_map\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3012, in _parsed_pkg_info\n",
      "    return self._pkg_info\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _pkg_info\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/commands/install.py\", line 568, in _warn_about_conflicts\n",
      "    package_set, _dep_info = check_install_conflicts(to_install)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/operations/check.py\", line 114, in check_install_conflicts\n",
      "    package_set, _ = create_package_set_from_installed()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/operations/check.py\", line 53, in create_package_set_from_installed\n",
      "    package_set[name] = PackageDetails(dist.version, dist.requires())\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2736, in requires\n",
      "    dm = self._dep_map\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3023, in _dep_map\n",
      "    self.__dep_map = self._compute_dependencies()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3032, in _compute_dependencies\n",
      "    for req in self._parsed_pkg_info.get_all('Requires-Dist') or []:\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3014, in _parsed_pkg_info\n",
      "    metadata = self.get_metadata(self.PKG_INFO)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 1420, in get_metadata\n",
      "    value = self._get(path)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 1616, in _get\n",
      "    with open(path, 'rb') as stream:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/lib/python3.8/dist-packages/optimum_habana-1.1.1.dist-info/METADATA'\n",
      "Successfully installed optimum-habana-1.2.0.dev0\n",
      "    Can't uninstall 'optimum-habana'. No files were found to uninstall.\n",
      "Installing collected packages: optimum-habana\n",
      "  Found existing installation: optimum-habana 1.1.1\n",
      "Installing collected packages: optimum-habana\n",
      "  Found existing installation: optimum-habana 1.1.1\n",
      "    Can't uninstall 'optimum-habana'. No files were found to uninstall.\n",
      "    Can't uninstall 'optimum-habana'. No files were found to uninstall.\n",
      "Installing collected packages: optimum-habana\n",
      "  Found existing installation: optimum-habana 1.1.1\n",
      "    Can't uninstall 'optimum-habana'. No files were found to uninstall.\n",
      "Successfully installed optimum-habana-1.2.0.dev0\n",
      "WARNING: You are using pip version 19.3.1; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "WARNING: You are using pip version 19.3.1; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "Successfully installed optimum-habana-1.2.0.dev0\n",
      "Successfully installed optimum-habana-1.2.0.dev0\n",
      "Successfully installed optimum-habana-1.2.0.dev0\n",
      "WARNING: You are using pip version 19.3.1; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "WARNING: You are using pip version 19.3.1; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "WARNING: You are using pip version 19.3.1; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "WARNING: You are using pip version 19.3.1; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "WARNING: You are using pip version 19.3.1; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "WARNING: You are using pip version 19.3.1; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "08/18/2022 07:00:51 - INFO - __main__ - Script parameters ScriptArguments(dataset_id='philschmid/processed_bert_dataset', tokenizer_id='philschmid/bert-base-uncased-2022-habana', repository_id='bert-base-uncased-2022-habana-test-6', hf_hub_token='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', model_config_id='bert-base-uncased', gaudi_config_id='philschmid/bert-base-uncased-2022-habana', per_device_train_batch_size=32, max_steps=70000, learning_rate=5e-05, mlm_probability=0.15)\n",
      "08/18/2022 07:00:51 - INFO - __main__ - Script parameters ScriptArguments(dataset_id='philschmid/processed_bert_dataset', tokenizer_id='philschmid/bert-base-uncased-2022-habana', repository_id='bert-base-uncased-2022-habana-test-6', hf_hub_token='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', model_config_id='bert-base-uncased', gaudi_config_id='philschmid/bert-base-uncased-2022-habana', per_device_train_batch_size=32, max_steps=70000, learning_rate=5e-05, mlm_probability=0.15)\n",
      "08/18/2022 07:00:51 - INFO - __main__ - Script parameters ScriptArguments(dataset_id='philschmid/processed_bert_dataset', tokenizer_id='philschmid/bert-base-uncased-2022-habana', repository_id='bert-base-uncased-2022-habana-test-6', hf_hub_token='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', model_config_id='bert-base-uncased', gaudi_config_id='philschmid/bert-base-uncased-2022-habana', per_device_train_batch_size=32, max_steps=70000, learning_rate=5e-05, mlm_probability=0.15)\n",
      "08/18/2022 07:00:51 - INFO - __main__ - Script parameters ScriptArguments(dataset_id='philschmid/processed_bert_dataset', tokenizer_id='philschmid/bert-base-uncased-2022-habana', repository_id='bert-base-uncased-2022-habana-test-6', hf_hub_token='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', model_config_id='bert-base-uncased', gaudi_config_id='philschmid/bert-base-uncased-2022-habana', per_device_train_batch_size=32, max_steps=70000, learning_rate=5e-05, mlm_probability=0.15)\n",
      "08/18/2022 07:00:51 - INFO - __main__ - Script parameters ScriptArguments(dataset_id='philschmid/processed_bert_dataset', tokenizer_id='philschmid/bert-base-uncased-2022-habana', repository_id='bert-base-uncased-2022-habana-test-6', hf_hub_token='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', model_config_id='bert-base-uncased', gaudi_config_id='philschmid/bert-base-uncased-2022-habana', per_device_train_batch_size=32, max_steps=70000, learning_rate=5e-05, mlm_probability=0.15)\n",
      "08/18/2022 07:00:51 - INFO - __main__ - Script parameters ScriptArguments(dataset_id='philschmid/processed_bert_dataset', tokenizer_id='philschmid/bert-base-uncased-2022-habana', repository_id='bert-base-uncased-2022-habana-test-6', hf_hub_token='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', model_config_id='bert-base-uncased', gaudi_config_id='philschmid/bert-base-uncased-2022-habana', per_device_train_batch_size=32, max_steps=70000, learning_rate=5e-05, mlm_probability=0.15)\n",
      "08/18/2022 07:00:51 - INFO - __main__ - Script parameters ScriptArguments(dataset_id='philschmid/processed_bert_dataset', tokenizer_id='philschmid/bert-base-uncased-2022-habana', repository_id='bert-base-uncased-2022-habana-test-6', hf_hub_token='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', model_config_id='bert-base-uncased', gaudi_config_id='philschmid/bert-base-uncased-2022-habana', per_device_train_batch_size=32, max_steps=70000, learning_rate=5e-05, mlm_probability=0.15)\n",
      "08/18/2022 07:00:51 - INFO - __main__ - Script parameters ScriptArguments(dataset_id='philschmid/processed_bert_dataset', tokenizer_id='philschmid/bert-base-uncased-2022-habana', repository_id='bert-base-uncased-2022-habana-test-6', hf_hub_token='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', model_config_id='bert-base-uncased', gaudi_config_id='philschmid/bert-base-uncased-2022-habana', per_device_train_batch_size=32, max_steps=70000, learning_rate=5e-05, mlm_probability=0.15)\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.07k/3.07k [00:00<00:00, 6.66MB/s]08/18/2022 07:00:52 - WARNING - datasets.builder - Using custom data configuration philschmid--processed_bert_dataset-595f30c0a502e611\n",
      "08/18/2022 07:00:52 - WARNING - datasets.builder - Using custom data configuration philschmid--processed_bert_dataset-595f30c0a502e611\n",
      "Downloading and preparing dataset None/None (download: 5.34 GiB, generated: 22.38 GiB, post-processed: Unknown size, total: 27.71 GiB) to /root/.cache/huggingface/datasets/philschmid___parquet/philschmid--processed_bert_dataset-595f30c0a502e611/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.07k/3.07k [00:00<00:00, 6.91MB/s]08/18/2022 07:00:52 - WARNING - datasets.builder - Using custom data configuration philschmid--processed_bert_dataset-595f30c0a502e611\n",
      "08/18/2022 07:00:52 - WARNING - datasets.builder - Using custom data configuration philschmid--processed_bert_dataset-595f30c0a502e611\n",
      "08/18/2022 07:00:52 - WARNING - datasets.builder - Using custom data configuration philschmid--processed_bert_dataset-595f30c0a502e611\n",
      "\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.07k/3.07k [00:00<00:00, 6.73MB/s]08/18/2022 07:00:52 - WARNING - datasets.builder - Using custom data configuration philschmid--processed_bert_dataset-595f30c0a502e611\n",
      "\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.07k/3.07k [00:00<00:00, 6.95MB/s]08/18/2022 07:00:52 - WARNING - datasets.builder - Using custom data configuration philschmid--processed_bert_dataset-595f30c0a502e611\n",
      "08/18/2022 07:00:52 - WARNING - datasets.builder - Using custom data configuration philschmid--processed_bert_dataset-595f30c0a502e611\n",
      "\n",
      "Downloading data:  11%|‚ñà         | 12.5M/117M [00:00<00:00, 125MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.6M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 38.4M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 51.4M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 64.5M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 77.5M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 90.5M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 104M/117M [00:00<00:00, 130MB/s] \u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.5M/117M [00:00<00:00, 125MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.5M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 38.3M/117M [00:00<00:00, 127MB/s]\u001b[A\n",
      "Downloading data:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 51.4M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 64.5M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 77.4M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 90.5M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 104M/117M [00:00<00:00, 130MB/s] \u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.8M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.8M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 38.8M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 51.8M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 64.9M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 77.9M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 90.9M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 130MB/s] \u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.8M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.9M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 39.0M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 52.1M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 65.1M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 78.1M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 91.2M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 130MB/s] \u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.6M/117M [00:00<00:00, 126MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.6M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 38.7M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 51.8M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 64.8M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 78.0M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 91.2M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 129MB/s] \u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.7M/117M [00:00<00:00, 127MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.3M/117M [00:00<00:00, 122MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 38.4M/117M [00:00<00:00, 126MB/s]\u001b[A\n",
      "Downloading data:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 51.3M/117M [00:00<00:00, 127MB/s]\u001b[A\n",
      "Downloading data:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 64.2M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 77.2M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 90.3M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 103M/117M [00:00<00:00, 130MB/s] \u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.4M/117M [00:00<00:00, 124MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.5M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 38.6M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 51.6M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 64.3M/117M [00:00<00:00, 122MB/s]\u001b[A\n",
      "Downloading data:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 76.6M/117M [00:00<00:00, 122MB/s]\u001b[A\n",
      "Downloading data:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 89.7M/117M [00:00<00:00, 125MB/s]\u001b[A\n",
      "Downloading data:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 103M/117M [00:00<00:00, 127MB/s] \u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 126MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.9M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 26.0M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 39.1M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 52.3M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 65.6M/117M [00:00<00:00, 132MB/s]\u001b[A\n",
      "Downloading data:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 78.8M/117M [00:00<00:00, 132MB/s]\u001b[A\n",
      "Downloading data:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 92.0M/117M [00:00<00:00, 132MB/s]\u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 131MB/s] \u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.6M/117M [00:00<00:00, 126MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.4M/117M [00:00<00:00, 127MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 38.3M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 51.2M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 64.2M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 77.1M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 90.0M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 103M/117M [00:00<00:00, 128MB/s] \u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.7M/117M [00:00<00:00, 127MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.7M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 38.7M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 51.7M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 64.7M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 77.7M/117M [00:00<00:00, 125MB/s]\u001b[A\n",
      "Downloading data:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 90.5M/117M [00:00<00:00, 126MB/s]\u001b[A\n",
      "Downloading data:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 104M/117M [00:00<00:00, 127MB/s] \u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.8M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.7M/117M [00:00<00:00, 127MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 38.5M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 51.3M/117M [00:00<00:00, 123MB/s]\u001b[A\n",
      "Downloading data:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 64.3M/117M [00:00<00:00, 126MB/s]\u001b[A\n",
      "Downloading data:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 77.2M/117M [00:00<00:00, 127MB/s]\u001b[A\n",
      "Downloading data:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 90.3M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 103M/117M [00:00<00:00, 128MB/s] \u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.7M/117M [00:00<00:00, 127MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.9M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 39.1M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 52.3M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 65.5M/117M [00:00<00:00, 132MB/s]\u001b[A\n",
      "Downloading data:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 78.7M/117M [00:00<00:00, 132MB/s]\u001b[A\n",
      "Downloading data:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 91.9M/117M [00:00<00:00, 132MB/s]\u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 131MB/s] \u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.8M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 26.0M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  34%|‚ñà‚ñà‚ñà‚ñé      | 39.2M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 52.3M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 65.3M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 78.5M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 91.7M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 131MB/s] \u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  10%|‚ñà         | 12.2M/117M [00:00<00:00, 122MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.4M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 38.6M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 51.9M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 65.0M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 78.2M/117M [00:00<00:00, 121MB/s]\u001b[A\n",
      "Downloading data:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 90.4M/117M [00:00<00:00, 121MB/s]\u001b[A\n",
      "Downloading data:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 103M/117M [00:00<00:00, 120MB/s] \u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 124MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  10%|‚ñâ         | 11.5M/117M [00:00<00:00, 115MB/s]\u001b[A\n",
      "Downloading data:  21%|‚ñà‚ñà        | 24.6M/117M [00:00<00:00, 124MB/s]\u001b[A\n",
      "Downloading data:  32%|‚ñà‚ñà‚ñà‚ñè      | 37.8M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 50.9M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 64.1M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 77.4M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 90.7M/117M [00:00<00:00, 132MB/s]\u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 130MB/s] \u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.6M/117M [00:00<00:00, 126MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.3M/117M [00:00<00:00, 126MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 38.1M/117M [00:00<00:00, 127MB/s]\u001b[A\n",
      "Downloading data:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 51.0M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 63.9M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 76.8M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 89.9M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 103M/117M [00:00<00:00, 129MB/s] \u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.8M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.8M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 38.9M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 51.8M/117M [00:00<00:00, 124MB/s]\u001b[A\n",
      "Downloading data:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 64.9M/117M [00:00<00:00, 126MB/s]\u001b[A\n",
      "Downloading data:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 77.8M/117M [00:00<00:00, 127MB/s]\u001b[A\n",
      "Downloading data:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 91.1M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 129MB/s] \u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.5M/117M [00:00<00:00, 125MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.7M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 38.8M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 51.8M/117M [00:00<00:00, 117MB/s]\u001b[A\n",
      "Downloading data:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 64.9M/117M [00:00<00:00, 122MB/s]\u001b[A\n",
      "Downloading data:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 78.2M/117M [00:00<00:00, 125MB/s]\u001b[A\n",
      "Downloading data:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 91.3M/117M [00:00<00:00, 127MB/s]\u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 127MB/s] \u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.3M/117M [00:00<00:00, 123MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.6M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 38.7M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 51.7M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 64.6M/117M [00:00<00:00, 124MB/s]\u001b[A\n",
      "Downloading data:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 77.9M/117M [00:00<00:00, 127MB/s]\u001b[A\n",
      "Downloading data:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 91.1M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 128MB/s] \u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.4M/117M [00:00<00:00, 124MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.1M/117M [00:00<00:00, 126MB/s]\u001b[A\n",
      "Downloading data:  32%|‚ñà‚ñà‚ñà‚ñè      | 37.8M/117M [00:00<00:00, 125MB/s]\u001b[A\n",
      "Downloading data:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 50.8M/117M [00:00<00:00, 127MB/s]\u001b[A\n",
      "Downloading data:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 63.9M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 76.9M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 90.0M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 103M/117M [00:00<00:00, 131MB/s] \u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.6M/117M [00:00<00:00, 126MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.2M/117M [00:00<00:00, 126MB/s]\u001b[A\n",
      "Downloading data:  32%|‚ñà‚ñà‚ñà‚ñè      | 38.0M/117M [00:00<00:00, 127MB/s]\u001b[A\n",
      "Downloading data:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 50.7M/117M [00:00<00:00, 127MB/s]\u001b[A\n",
      "Downloading data:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 63.5M/117M [00:00<00:00, 127MB/s]\u001b[A\n",
      "Downloading data:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 76.2M/117M [00:00<00:00, 127MB/s]\u001b[A\n",
      "Downloading data:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 88.9M/117M [00:00<00:00, 127MB/s]\u001b[A\n",
      "Downloading data:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 102M/117M [00:00<00:00, 127MB/s] \u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 127MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.6M/117M [00:00<00:00, 126MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.4M/117M [00:00<00:00, 127MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 38.5M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 51.5M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 64.6M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 77.6M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 90.6M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 104M/117M [00:00<00:00, 130MB/s] \u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.8M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.9M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 38.9M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 52.0M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 65.4M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 78.6M/117M [00:00<00:00, 132MB/s]\u001b[A\n",
      "Downloading data:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 91.8M/117M [00:00<00:00, 132MB/s]\u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 131MB/s] \u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.7M/117M [00:00<00:00, 127MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.8M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 38.8M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 52.0M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 65.0M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 78.0M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 91.2M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 129MB/s] \u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.4M/117M [00:00<00:00, 124MB/s]\u001b[A\n",
      "Downloading data:  21%|‚ñà‚ñà        | 24.8M/117M [00:00<00:00, 121MB/s]\u001b[A\n",
      "Downloading data:  32%|‚ñà‚ñà‚ñà‚ñè      | 36.9M/117M [00:00<00:00, 121MB/s]\u001b[A\n",
      "Downloading data:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 49.0M/117M [00:00<00:00, 121MB/s]\u001b[A\n",
      "Downloading data:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 61.2M/117M [00:00<00:00, 121MB/s]\u001b[A\n",
      "Downloading data:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 73.4M/117M [00:00<00:00, 121MB/s]\u001b[A\n",
      "Downloading data:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 85.6M/117M [00:00<00:00, 122MB/s]\u001b[A\n",
      "Downloading data:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 97.8M/117M [00:00<00:00, 122MB/s]\u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 122MB/s] \u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  10%|‚ñà         | 11.9M/117M [00:00<00:00, 119MB/s]\u001b[A\n",
      "Downloading data:  21%|‚ñà‚ñà        | 24.1M/117M [00:00<00:00, 121MB/s]\u001b[A\n",
      "Downloading data:  31%|‚ñà‚ñà‚ñà       | 36.3M/117M [00:00<00:00, 121MB/s]\u001b[A\n",
      "Downloading data:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 48.4M/117M [00:00<00:00, 121MB/s]\u001b[A\n",
      "Downloading data:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 60.6M/117M [00:00<00:00, 121MB/s]\u001b[A\n",
      "Downloading data:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 72.8M/117M [00:00<00:00, 122MB/s]\u001b[A\n",
      "Downloading data:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 84.9M/117M [00:00<00:00, 121MB/s]\u001b[A\n",
      "Downloading data:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 97.0M/117M [00:00<00:00, 121MB/s]\u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 121MB/s] \u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  10%|‚ñâ         | 11.5M/117M [00:00<00:00, 115MB/s]\u001b[A\n",
      "Downloading data:  20%|‚ñà‚ñà        | 23.8M/117M [00:00<00:00, 120MB/s]\u001b[A\n",
      "Downloading data:  31%|‚ñà‚ñà‚ñà       | 36.1M/117M [00:00<00:00, 121MB/s]\u001b[A\n",
      "Downloading data:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 48.5M/117M [00:00<00:00, 122MB/s]\u001b[A\n",
      "Downloading data:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 60.8M/117M [00:00<00:00, 123MB/s]\u001b[A\n",
      "Downloading data:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 73.2M/117M [00:00<00:00, 123MB/s]\u001b[A\n",
      "Downloading data:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 85.6M/117M [00:00<00:00, 123MB/s]\u001b[A\n",
      "Downloading data:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 98.0M/117M [00:00<00:00, 123MB/s]\u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 122MB/s] \u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.7M/117M [00:00<00:00, 127MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.8M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 39.0M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 52.0M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 65.1M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 78.3M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 91.5M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 131MB/s] \u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.8M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.9M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  34%|‚ñà‚ñà‚ñà‚ñé      | 39.1M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 52.3M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 65.5M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 78.6M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 91.8M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 131MB/s] \u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.8M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.9M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 38.8M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 51.7M/117M [00:00<00:00, 121MB/s]\u001b[A\n",
      "Downloading data:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 64.0M/117M [00:00<00:00, 120MB/s]\u001b[A\n",
      "Downloading data:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 77.0M/117M [00:00<00:00, 123MB/s]\u001b[A\n",
      "Downloading data:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 90.1M/117M [00:00<00:00, 126MB/s]\u001b[A\n",
      "Downloading data:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 103M/117M [00:00<00:00, 128MB/s] \u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 126MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.6M/117M [00:00<00:00, 126MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.5M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 38.7M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 51.9M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 64.9M/117M [00:00<00:00, 124MB/s]\u001b[A\n",
      "Downloading data:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 78.2M/117M [00:00<00:00, 127MB/s]\u001b[A\n",
      "Downloading data:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 91.2M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 104M/117M [00:00<00:00, 128MB/s] \u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.6M/117M [00:00<00:00, 126MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.5M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 38.6M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 51.5M/117M [00:00<00:00, 127MB/s]\u001b[A\n",
      "Downloading data:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 64.6M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 77.8M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 90.9M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 130MB/s] \u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.6M/117M [00:00<00:00, 126MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.6M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 38.6M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 51.7M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 64.7M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 77.7M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 90.7M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 104M/117M [00:00<00:00, 130MB/s] \u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.9M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 26.2M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data:  34%|‚ñà‚ñà‚ñà‚ñç      | 39.6M/117M [00:00<00:00, 133MB/s]\u001b[A\n",
      "Downloading data:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 52.9M/117M [00:00<00:00, 132MB/s]\u001b[A\n",
      "Downloading data:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 66.1M/117M [00:00<00:00, 132MB/s]\u001b[A\n",
      "Downloading data:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 79.3M/117M [00:00<00:00, 132MB/s]\u001b[A\n",
      "Downloading data:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 92.6M/117M [00:00<00:00, 132MB/s]\u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 132MB/s] \u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.8M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.9M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 39.2M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 52.5M/117M [00:00<00:00, 132MB/s]\u001b[A\n",
      "Downloading data:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 65.8M/117M [00:00<00:00, 132MB/s]\u001b[A\n",
      "Downloading data:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 79.0M/117M [00:00<00:00, 132MB/s]\u001b[A\n",
      "Downloading data:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 92.1M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 131MB/s] \u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.6M/117M [00:00<00:00, 126MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.7M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 38.5M/117M [00:00<00:00, 127MB/s]\u001b[A\n",
      "Downloading data:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 51.7M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 64.9M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 78.2M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 91.6M/117M [00:00<00:00, 132MB/s]\u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 130MB/s] \u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.6M/117M [00:00<00:00, 126MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.4M/117M [00:00<00:00, 127MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 38.3M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 51.2M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 64.2M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 77.3M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 90.3M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 103M/117M [00:00<00:00, 129MB/s] \u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.7M/117M [00:00<00:00, 127MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.8M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 39.0M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 52.1M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 65.2M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 78.4M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 91.5M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 130MB/s] \u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.9M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 26.1M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data:  34%|‚ñà‚ñà‚ñà‚ñé      | 39.4M/117M [00:00<00:00, 132MB/s]\u001b[A\n",
      "Downloading data:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 52.6M/117M [00:00<00:00, 132MB/s]\u001b[A\n",
      "Downloading data:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 65.9M/117M [00:00<00:00, 132MB/s]\u001b[A\n",
      "Downloading data:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 79.2M/117M [00:00<00:00, 133MB/s]\u001b[A\n",
      "Downloading data:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 92.5M/117M [00:00<00:00, 133MB/s]\u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 129MB/s] \u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.6M/117M [00:00<00:00, 126MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.8M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 39.0M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 52.2M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 65.3M/117M [00:00<00:00, 124MB/s]\u001b[A\n",
      "Downloading data:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 78.4M/117M [00:00<00:00, 126MB/s]\u001b[A\n",
      "Downloading data:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 91.5M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 128MB/s] \u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.6M/117M [00:00<00:00, 126MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.8M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 38.7M/117M [00:00<00:00, 127MB/s]\u001b[A\n",
      "Downloading data:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 51.6M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 64.7M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 77.8M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 90.9M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 104M/117M [00:00<00:00, 130MB/s] \u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.6M/117M [00:00<00:00, 126MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.7M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 38.9M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 51.9M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 64.9M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 77.9M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 90.9M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 129MB/s] \u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.5M/117M [00:00<00:00, 125MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.2M/117M [00:00<00:00, 126MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 38.1M/117M [00:00<00:00, 127MB/s]\u001b[A\n",
      "Downloading data:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 51.1M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 63.9M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 76.9M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 89.8M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 103M/117M [00:00<00:00, 129MB/s] \u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.8M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.9M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 39.0M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 52.2M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 65.3M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 78.5M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 91.7M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 131MB/s] \u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.6M/117M [00:00<00:00, 126MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.5M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 38.3M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 51.1M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 63.9M/117M [00:00<00:01, 49.8MB/s]\u001b[A\n",
      "Downloading data:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 76.0M/117M [00:01<00:00, 61.6MB/s]\u001b[A\n",
      "Downloading data:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 88.6M/117M [00:01<00:00, 74.0MB/s]\u001b[A\n",
      "Downloading data:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 101M/117M [00:01<00:00, 85.7MB/s] \u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:01<00:00, 85.9MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.7M/117M [00:00<00:00, 127MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.8M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 38.9M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 52.0M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 65.2M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 78.3M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 91.4M/117M [00:00<00:00, 127MB/s]\u001b[A\n",
      "Downloading data:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 104M/117M [00:00<00:00, 114MB/s] \u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:01<00:00, 115MB/s] \u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.8M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 26.0M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 39.0M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 52.2M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 65.3M/117M [00:00<00:00, 131MB/s]\u001b[A\n",
      "Downloading data:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 78.3M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 91.4M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 130MB/s] \u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.4M/117M [00:00<00:00, 124MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.2M/117M [00:00<00:00, 126MB/s]\u001b[A\n",
      "Downloading data:  32%|‚ñà‚ñà‚ñà‚ñè      | 37.8M/117M [00:00<00:00, 126MB/s]\u001b[A\n",
      "Downloading data:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 50.7M/117M [00:00<00:00, 127MB/s]\u001b[A\n",
      "Downloading data:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 63.6M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 76.5M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 89.4M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 102M/117M [00:00<00:00, 129MB/s] \u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|‚ñà         | 12.7M/117M [00:00<00:00, 127MB/s]\u001b[A\n",
      "Downloading data:  22%|‚ñà‚ñà‚ñè       | 25.6M/117M [00:00<00:00, 128MB/s]\u001b[A\n",
      "Downloading data:  33%|‚ñà‚ñà‚ñà‚ñé      | 38.6M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 51.5M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 64.5M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 77.5M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 90.5M/117M [00:00<00:00, 130MB/s]\u001b[A\n",
      "Downloading data:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 104M/117M [00:00<00:00, 130MB/s] \u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117M/117M [00:00<00:00, 129MB/s]\u001b[A\n",
      "Downloading data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:49<00:00, 49.07s/it]\n",
      "Extracting data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.87it/s]\n",
      "                                  Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/philschmid___parquet/philschmid--processed_bert_dataset-595f30c0a502e611/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "08/18/2022 07:02:47 - WARNING - datasets.builder - Reusing dataset parquet (/root/.cache/huggingface/datasets/philschmid___parquet/philschmid--processed_bert_dataset-595f30c0a502e611/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "08/18/2022 07:02:47 - WARNING - datasets.builder - Reusing dataset parquet (/root/.cache/huggingface/datasets/philschmid___parquet/philschmid--processed_bert_dataset-595f30c0a502e611/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "08/18/2022 07:02:47 - WARNING - datasets.builder - Reusing dataset parquet (/root/.cache/huggingface/datasets/philschmid___parquet/philschmid--processed_bert_dataset-595f30c0a502e611/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Downloading tokenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 371/371 [00:00<00:00, 801kB/s]08/18/2022 07:02:47 - WARNING - datasets.builder - Reusing dataset parquet (/root/.cache/huggingface/datasets/philschmid___parquet/philschmid--processed_bert_dataset-595f30c0a502e611/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "08/18/2022 07:02:47 - WARNING - datasets.builder - Reusing dataset parquet (/root/.cache/huggingface/datasets/philschmid___parquet/philschmid--processed_bert_dataset-595f30c0a502e611/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "08/18/2022 07:02:47 - WARNING - datasets.builder - Reusing dataset parquet (/root/.cache/huggingface/datasets/philschmid___parquet/philschmid--processed_bert_dataset-595f30c0a502e611/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "\n",
      "Downloading vocab.txt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 153k/153k [00:00<00:00, 92.5MB/s]08/18/2022 07:02:47 - WARNING - datasets.builder - Reusing dataset parquet (/root/.cache/huggingface/datasets/philschmid___parquet/philschmid--processed_bert_dataset-595f30c0a502e611/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "\n",
      "Downloading special_tokens_map.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:00<00:00, 321kB/s]08/18/2022 07:02:47 - INFO - __main__ - Training new model from scratch\n",
      "08/18/2022 07:02:47 - INFO - __main__ - Training new model from scratch\n",
      "08/18/2022 07:02:47 - INFO - __main__ - Training new model from scratch\n",
      "08/18/2022 07:02:47 - INFO - __main__ - Training new model from scratch\n",
      "08/18/2022 07:02:47 - INFO - __main__ - Training new model from scratch\n",
      "08/18/2022 07:02:47 - INFO - __main__ - Training new model from scratch\n",
      "08/18/2022 07:02:47 - INFO - __main__ - Training new model from scratch\n",
      "\n",
      "Downloading config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 570/570 [00:00<00:00, 1.17MB/s]08/18/2022 07:02:47 - INFO - __main__ - Training new model from scratch\n",
      "08/18/2022 07:02:49 - INFO - __main__ - Resizing token embedding to 32000\n",
      "08/18/2022 07:02:49 - INFO - __main__ - Resizing token embedding to 32000\n",
      "08/18/2022 07:02:49 - INFO - __main__ - Resizing token embedding to 32000\n",
      "08/18/2022 07:02:49 - INFO - __main__ - Resizing token embedding to 32000\n",
      "08/18/2022 07:02:49 - INFO - __main__ - Resizing token embedding to 32000\n",
      "08/18/2022 07:02:49 - INFO - __main__ - Resizing token embedding to 32000\n",
      "08/18/2022 07:02:49 - INFO - __main__ - Resizing token embedding to 32000\n",
      "08/18/2022 07:02:49 - INFO - __main__ - Resizing token embedding to 32000\n",
      "\n",
      "PyTorch: setting up devices\n",
      "\n",
      "PyTorch: setting up devices\n",
      "Habana is enabled.\n",
      "Enabled lazy mode.\n",
      "\n",
      "PyTorch: setting up devices\n",
      "PyTorch: setting up devices\n",
      "PyTorch: setting up devices\n",
      "\n",
      "PyTorch: setting up devices\n",
      "PyTorch: setting up devices\n",
      "\n",
      "PyTorch: setting up devices\n",
      "Habana is enabled.\n",
      "Enabled lazy mode.\n",
      "Habana is enabled.\n",
      "Enabled lazy mode.\n",
      "Habana is enabled.\n",
      "Enabled lazy mode.\n",
      "Habana is enabled.\n",
      "Enabled lazy mode.\n",
      "Habana is enabled.\n",
      "Enabled lazy mode.\n",
      "Habana is enabled.\n",
      "Enabled lazy mode.\n",
      "Habana is enabled.\n",
      "Enabled lazy mode.\n",
      "08/18/2022 07:02:50 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 2\n",
      "08/18/2022 07:02:50 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 6\n",
      "08/18/2022 07:02:50 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1\n",
      "08/18/2022 07:02:51 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 4\n",
      "08/18/2022 07:02:51 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 5\n",
      "08/18/2022 07:02:51 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 7\n",
      "08/18/2022 07:02:51 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 3\n",
      "08/18/2022 07:02:51 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "08/18/2022 07:02:51 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\n",
      "08/18/2022 07:02:51 - INFO - torch.distributed.distributed_c10d - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\n",
      "Enabled distributed run.\n",
      "Enabled distributed run.\n",
      "Enabled distributed run.\n",
      "Enabled distributed run.\n",
      "Enabled distributed run.\n",
      "08/18/2022 07:02:51 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\n",
      "08/18/2022 07:02:51 - INFO - torch.distributed.distributed_c10d - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\n",
      "08/18/2022 07:02:51 - INFO - torch.distributed.distributed_c10d - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\n",
      "08/18/2022 07:02:51 - INFO - torch.distributed.distributed_c10d - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\n",
      "08/18/2022 07:02:51 - INFO - torch.distributed.distributed_c10d - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\n",
      "08/18/2022 07:02:51 - INFO - torch.distributed.distributed_c10d - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\n",
      "Enabled distributed run.\n",
      "Enabled distributed run.\n",
      "Enabled distributed run.\n",
      "08/18/2022 07:02:53 - WARNING - huggingface_hub.repository - Cloning https://huggingface.co/philschmid/bert-base-uncased-2022-habana-test-6 into local empty directory.\n",
      "Cloning https://huggingface.co/philschmid/bert-base-uncased-2022-habana-test-6 into local empty directory.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Downloading gaudi_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 520/520 [00:00<00:00, 1.40MB/s]\n",
      "loading configuration file https://huggingface.co/philschmid/bert-base-uncased-2022-habana/resolve/main/gaudi_config.json from cache at /root/.cache/huggingface/transformers/fb56938113c92d300b4fc64972dc73cd8ec527700bb5c74087d494ee0d6fc513.b70269b061031887e01b69a90e4577eda72df92cf3dfb44c1c75b1bed8e7d3ae\n",
      "GaudiConfig {\n",
      "  \"hmp_bf16_ops\": [\n",
      "    \"add\",\n",
      "    \"addmm\",\n",
      "    \"bmm\",\n",
      "    \"div\",\n",
      "    \"dropout\",\n",
      "    \"gelu\",\n",
      "    \"iadd\",\n",
      "    \"linear\",\n",
      "    \"layer_norm\",\n",
      "    \"matmul\",\n",
      "    \"mm\",\n",
      "    \"rsub\",\n",
      "    \"softmax\",\n",
      "    \"truediv\"\n",
      "  ],\n",
      "  \"hmp_fp32_ops\": [\n",
      "    \"embedding\",\n",
      "    \"nll_loss\",\n",
      "    \"log_softmax\",\n",
      "    \"cross_entropy\"\n",
      "  ],\n",
      "  \"hmp_is_verbose\": false,\n",
      "  \"hmp_opt_level\": \"O1\",\n",
      "  \"optimum_version\": \"1.3.0\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"use_fused_adam\": true,\n",
      "  \"use_fused_clip_norm\": true,\n",
      "  \"use_habana_mixed_precision\": true\n",
      "}\n",
      "\n",
      "hmp:verbose_mode  False\n",
      "hmp:opt_level O1\n",
      "loading configuration file https://huggingface.co/philschmid/bert-base-uncased-2022-habana/resolve/main/gaudi_config.json from cache at /root/.cache/huggingface/transformers/fb56938113c92d300b4fc64972dc73cd8ec527700bb5c74087d494ee0d6fc513.b70269b061031887e01b69a90e4577eda72df92cf3dfb44c1c75b1bed8e7d3ae\n",
      "loading configuration file https://huggingface.co/philschmid/bert-base-uncased-2022-habana/resolve/main/gaudi_config.json from cache at /root/.cache/huggingface/transformers/fb56938113c92d300b4fc64972dc73cd8ec527700bb5c74087d494ee0d6fc513.b70269b061031887e01b69a90e4577eda72df92cf3dfb44c1c75b1bed8e7d3ae\n",
      "GaudiConfig {\n",
      "  \"hmp_bf16_ops\": [\n",
      "    \"add\",\n",
      "    \"addmm\",\n",
      "    \"bmm\",\n",
      "    \"div\",\n",
      "    \"dropout\",\n",
      "    \"gelu\",\n",
      "    \"iadd\",\n",
      "    \"linear\",\n",
      "    \"layer_norm\",\n",
      "    \"matmul\",\n",
      "    \"mm\",\n",
      "    \"rsub\",\n",
      "    \"softmax\",\n",
      "    \"truediv\"\n",
      "  ],\n",
      "  \"hmp_fp32_ops\": [\n",
      "    \"embedding\",\n",
      "    \"nll_loss\",\n",
      "    \"log_softmax\",\n",
      "    \"cross_entropy\"\n",
      "  ],\n",
      "  \"hmp_is_verbose\": false,\n",
      "  \"hmp_opt_level\": \"O1\",\n",
      "  \"optimum_version\": \"1.3.0\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"use_fused_adam\": true,\n",
      "  \"use_fused_clip_norm\": true,\n",
      "  \"use_habana_mixed_precision\": true\n",
      "}\n",
      "\n",
      "GaudiConfig {\n",
      "  \"hmp_bf16_ops\": [\n",
      "    \"add\",\n",
      "    \"addmm\",\n",
      "    \"bmm\",\n",
      "    \"div\",\n",
      "    \"dropout\",\n",
      "    \"gelu\",\n",
      "    \"iadd\",\n",
      "    \"linear\",\n",
      "    \"layer_norm\",\n",
      "    \"matmul\",\n",
      "    \"mm\",\n",
      "    \"rsub\",\n",
      "    \"softmax\",\n",
      "    \"truediv\"\n",
      "  ],\n",
      "  \"hmp_fp32_ops\": [\n",
      "    \"embedding\",\n",
      "    \"nll_loss\",\n",
      "    \"log_softmax\",\n",
      "    \"cross_entropy\"\n",
      "  ],\n",
      "  \"hmp_is_verbose\": false,\n",
      "  \"hmp_opt_level\": \"O1\",\n",
      "  \"optimum_version\": \"1.3.0\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"use_fused_adam\": true,\n",
      "  \"use_fused_clip_norm\": true,\n",
      "  \"use_habana_mixed_precision\": true\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/philschmid/bert-base-uncased-2022-habana/resolve/main/gaudi_config.json from cache at /root/.cache/huggingface/transformers/fb56938113c92d300b4fc64972dc73cd8ec527700bb5c74087d494ee0d6fc513.b70269b061031887e01b69a90e4577eda72df92cf3dfb44c1c75b1bed8e7d3ae\n",
      "GaudiConfig {\n",
      "  \"hmp_bf16_ops\": [\n",
      "    \"add\",\n",
      "    \"addmm\",\n",
      "    \"bmm\",\n",
      "    \"div\",\n",
      "    \"dropout\",\n",
      "    \"gelu\",\n",
      "    \"iadd\",\n",
      "    \"linear\",\n",
      "    \"layer_norm\",\n",
      "    \"matmul\",\n",
      "    \"mm\",\n",
      "    \"rsub\",\n",
      "    \"softmax\",\n",
      "    \"truediv\"\n",
      "  ],\n",
      "  \"hmp_fp32_ops\": [\n",
      "    \"embedding\",\n",
      "    \"nll_loss\",\n",
      "    \"log_softmax\",\n",
      "    \"cross_entropy\"\n",
      "  ],\n",
      "  \"hmp_is_verbose\": false,\n",
      "  \"hmp_opt_level\": \"O1\",\n",
      "  \"optimum_version\": \"1.3.0\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"use_fused_adam\": true,\n",
      "  \"use_fused_clip_norm\": true,\n",
      "  \"use_habana_mixed_precision\": true\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/philschmid/bert-base-uncased-2022-habana/resolve/main/gaudi_config.json from cache at /root/.cache/huggingface/transformers/fb56938113c92d300b4fc64972dc73cd8ec527700bb5c74087d494ee0d6fc513.b70269b061031887e01b69a90e4577eda72df92cf3dfb44c1c75b1bed8e7d3ae\n",
      "loading configuration file https://huggingface.co/philschmid/bert-base-uncased-2022-habana/resolve/main/gaudi_config.json from cache at /root/.cache/huggingface/transformers/fb56938113c92d300b4fc64972dc73cd8ec527700bb5c74087d494ee0d6fc513.b70269b061031887e01b69a90e4577eda72df92cf3dfb44c1c75b1bed8e7d3ae\n",
      "GaudiConfig {\n",
      "  \"hmp_bf16_ops\": [\n",
      "    \"add\",\n",
      "    \"addmm\",\n",
      "    \"bmm\",\n",
      "    \"div\",\n",
      "    \"dropout\",\n",
      "    \"gelu\",\n",
      "    \"iadd\",\n",
      "    \"linear\",\n",
      "    \"layer_norm\",\n",
      "    \"matmul\",\n",
      "    \"mm\",\n",
      "    \"rsub\",\n",
      "    \"softmax\",\n",
      "    \"truediv\"\n",
      "  ],\n",
      "  \"hmp_fp32_ops\": [\n",
      "    \"embedding\",\n",
      "    \"nll_loss\",\n",
      "    \"log_softmax\",\n",
      "    \"cross_entropy\"\n",
      "  ],\n",
      "  \"hmp_is_verbose\": false,\n",
      "  \"hmp_opt_level\": \"O1\",\n",
      "  \"optimum_version\": \"1.3.0\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"use_fused_adam\": true,\n",
      "  \"use_fused_clip_norm\": true,\n",
      "  \"use_habana_mixed_precision\": true\n",
      "}\n",
      "\n",
      "GaudiConfig {\n",
      "  \"hmp_bf16_ops\": [\n",
      "    \"add\",\n",
      "    \"addmm\",\n",
      "    \"bmm\",\n",
      "    \"div\",\n",
      "    \"dropout\",\n",
      "    \"gelu\",\n",
      "    \"iadd\",\n",
      "    \"linear\",\n",
      "    \"layer_norm\",\n",
      "    \"matmul\",\n",
      "    \"mm\",\n",
      "    \"rsub\",\n",
      "    \"softmax\",\n",
      "    \"truediv\"\n",
      "  ],\n",
      "  \"hmp_fp32_ops\": [\n",
      "    \"embedding\",\n",
      "    \"nll_loss\",\n",
      "    \"log_softmax\",\n",
      "    \"cross_entropy\"\n",
      "  ],\n",
      "  \"hmp_is_verbose\": false,\n",
      "  \"hmp_opt_level\": \"O1\",\n",
      "  \"optimum_version\": \"1.3.0\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"use_fused_adam\": true,\n",
      "  \"use_fused_clip_norm\": true,\n",
      "  \"use_habana_mixed_precision\": true\n",
      "}\n",
      "\n",
      "hmp:verbose_mode  False\n",
      "hmp:opt_level O1\n",
      "hmp:verbose_mode  False\n",
      "hmp:opt_level O1\n",
      "hmp:verbose_mode  False\n",
      "hmp:opt_level O1\n",
      "hmp:verbose_mode  False\n",
      "hmp:opt_level O1\n",
      "hmp:verbose_mode  False\n",
      "hmp:opt_level O1\n",
      "loading configuration file https://huggingface.co/philschmid/bert-base-uncased-2022-habana/resolve/main/gaudi_config.json from cache at /root/.cache/huggingface/transformers/fb56938113c92d300b4fc64972dc73cd8ec527700bb5c74087d494ee0d6fc513.b70269b061031887e01b69a90e4577eda72df92cf3dfb44c1c75b1bed8e7d3ae\n",
      "loading configuration file https://huggingface.co/philschmid/bert-base-uncased-2022-habana/resolve/main/gaudi_config.json from cache at /root/.cache/huggingface/transformers/fb56938113c92d300b4fc64972dc73cd8ec527700bb5c74087d494ee0d6fc513.b70269b061031887e01b69a90e4577eda72df92cf3dfb44c1c75b1bed8e7d3ae\n",
      "GaudiConfig {\n",
      "  \"hmp_bf16_ops\": [\n",
      "    \"add\",\n",
      "    \"addmm\",\n",
      "    \"bmm\",\n",
      "    \"div\",\n",
      "    \"dropout\",\n",
      "    \"gelu\",\n",
      "    \"iadd\",\n",
      "    \"linear\",\n",
      "    \"layer_norm\",\n",
      "    \"matmul\",\n",
      "    \"mm\",\n",
      "    \"rsub\",\n",
      "    \"softmax\",\n",
      "    \"truediv\"\n",
      "  ],\n",
      "  \"hmp_fp32_ops\": [\n",
      "    \"embedding\",\n",
      "    \"nll_loss\",\n",
      "    \"log_softmax\",\n",
      "    \"cross_entropy\"\n",
      "  ],\n",
      "  \"hmp_is_verbose\": false,\n",
      "  \"hmp_opt_level\": \"O1\",\n",
      "  \"optimum_version\": \"1.3.0\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"use_fused_adam\": true,\n",
      "  \"use_fused_clip_norm\": true,\n",
      "  \"use_habana_mixed_precision\": true\n",
      "}\n",
      "\n",
      "GaudiConfig {\n",
      "  \"hmp_bf16_ops\": [\n",
      "    \"add\",\n",
      "    \"addmm\",\n",
      "    \"bmm\",\n",
      "    \"div\",\n",
      "    \"dropout\",\n",
      "    \"gelu\",\n",
      "    \"iadd\",\n",
      "    \"linear\",\n",
      "    \"layer_norm\",\n",
      "    \"matmul\",\n",
      "    \"mm\",\n",
      "    \"rsub\",\n",
      "    \"softmax\",\n",
      "    \"truediv\"\n",
      "  ],\n",
      "  \"hmp_fp32_ops\": [\n",
      "    \"embedding\",\n",
      "    \"nll_loss\",\n",
      "    \"log_softmax\",\n",
      "    \"cross_entropy\"\n",
      "  ],\n",
      "  \"hmp_is_verbose\": false,\n",
      "  \"hmp_opt_level\": \"O1\",\n",
      "  \"optimum_version\": \"1.3.0\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"use_fused_adam\": true,\n",
      "  \"use_fused_clip_norm\": true,\n",
      "  \"use_habana_mixed_precision\": true\n",
      "}\n",
      "\n",
      "hmp:verbose_mode  False\n",
      "hmp:opt_level O1\n",
      "hmp:verbose_mode  False\n",
      "hmp:opt_level O1\n",
      "The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 6674250\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 70000\n",
      "  Number of trainable parameters = 135258880\n",
      "  0%|          | 100/70000 [01:19<9:18:51,  2.08it/s]{'loss': 8.36, 'learning_rate': 4.992857142857143e-05, 'epoch': 0.0}\n",
      "{'loss': 6.76, 'learning_rate': 4.985714285714286e-05, 'epoch': 0.01}\n",
      "{'loss': 6.56, 'learning_rate': 4.978571428571429e-05, 'epoch': 0.01}\n",
      "  1%|          | 400/70000 [03:45<9:32:05,  2.03it/s]{'loss': 6.44, 'learning_rate': 4.971428571428572e-05, 'epoch': 0.02}\n",
      "{'loss': 6.4, 'learning_rate': 4.964285714285715e-05, 'epoch': 0.02}\n",
      "  1%|          | 600/70000 [05:24<9:41:00,  1.99it/s]{'loss': 6.36, 'learning_rate': 4.957142857142857e-05, 'epoch': 0.02}\n",
      "  1%|          | 700/70000 [06:13<9:40:04,  1.99it/s] {'loss': 6.36, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.03}\n",
      "{'loss': 6.32, 'learning_rate': 4.942857142857143e-05, 'epoch': 0.03}\n",
      "  1%|‚ñè         | 900/70000 [07:52<9:15:57,  2.07it/s]{'loss': 6.32, 'learning_rate': 4.935714285714286e-05, 'epoch': 0.03}\n",
      "{'loss': 6.28, 'learning_rate': 4.928571428571429e-05, 'epoch': 0.04}\n",
      "  2%|‚ñè         | 1100/70000 [09:28<9:12:57,  2.08it/s]{'loss': 6.28, 'learning_rate': 4.921428571428572e-05, 'epoch': 0.04}\n",
      "{'loss': 6.28, 'learning_rate': 4.9142857142857144e-05, 'epoch': 0.05}\n",
      "{'loss': 6.2, 'learning_rate': 4.9071428571428574e-05, 'epoch': 0.05}\n",
      "{'loss': 6.2, 'learning_rate': 4.9e-05, 'epoch': 0.05}\n",
      "  2%|‚ñè         | 1500/70000 [12:42<9:12:24,  2.07it/s]{'loss': 6.24, 'learning_rate': 4.892857142857143e-05, 'epoch': 0.06}\n",
      "{'loss': 6.16, 'learning_rate': 4.885714285714286e-05, 'epoch': 0.06}\n",
      "{'loss': 6.12, 'learning_rate': 4.878571428571429e-05, 'epoch': 0.07}\n",
      "{'loss': 6.16, 'learning_rate': 4.8714285714285714e-05, 'epoch': 0.07}\n",
      "{'loss': 6.12, 'learning_rate': 4.8642857142857145e-05, 'epoch': 0.07}\n",
      "{'loss': 6.16, 'learning_rate': 4.8571428571428576e-05, 'epoch': 0.08}\n",
      "{'loss': 6.08, 'learning_rate': 4.85e-05, 'epoch': 0.08}\n",
      "{'loss': 6.12, 'learning_rate': 4.842857142857143e-05, 'epoch': 0.08}\n",
      "  3%|‚ñé         | 2300/70000 [19:09<9:03:10,  2.08it/s]{'loss': 6.04, 'learning_rate': 4.835714285714286e-05, 'epoch': 0.09}\n",
      "{'loss': 6.0, 'learning_rate': 4.828571428571429e-05, 'epoch': 0.09}\n",
      "{'loss': 6.0, 'learning_rate': 4.8214285714285716e-05, 'epoch': 0.1}\n",
      "  4%|‚ñé         | 2600/70000 [21:37<9:01:26,  2.07it/s] {'loss': 6.04, 'learning_rate': 4.8142857142857147e-05, 'epoch': 0.1}\n",
      "{'loss': 6.0, 'learning_rate': 4.807142857142857e-05, 'epoch': 0.1}\n",
      "{'loss': 5.96, 'learning_rate': 4.8e-05, 'epoch': 0.11}\n",
      "{'loss': 5.96, 'learning_rate': 4.7928571428571425e-05, 'epoch': 0.11}\n",
      "{'loss': 6.0, 'learning_rate': 4.785714285714286e-05, 'epoch': 0.12}\n",
      "  4%|‚ñç         | 3100/70000 [25:40<8:56:28,  2.08it/s]{'loss': 5.96, 'learning_rate': 4.7785714285714287e-05, 'epoch': 0.12}\n",
      "  5%|‚ñç         | 3200/70000 [26:29<8:57:47,  2.07it/s]{'loss': 5.92, 'learning_rate': 4.771428571428572e-05, 'epoch': 0.12}\n",
      "{'loss': 5.92, 'learning_rate': 4.764285714285715e-05, 'epoch': 0.13}\n",
      "{'loss': 5.92, 'learning_rate': 4.757142857142857e-05, 'epoch': 0.13}\n",
      "{'loss': 5.92, 'learning_rate': 4.75e-05, 'epoch': 0.13}\n",
      "  5%|‚ñå         | 3600/70000 [29:43<8:54:11,  2.07it/s]{'loss': 5.92, 'learning_rate': 4.742857142857143e-05, 'epoch': 0.14}\n",
      "{'loss': 5.88, 'learning_rate': 4.7357142857142864e-05, 'epoch': 0.14}\n",
      "  5%|‚ñå         | 3800/70000 [31:20<9:12:53,  2.00it/s] {'loss': 5.88, 'learning_rate': 4.728571428571429e-05, 'epoch': 0.15}\n",
      "  6%|‚ñå         | 3900/70000 [32:09<8:50:11,  2.08it/s]{'loss': 5.88, 'learning_rate': 4.721428571428572e-05, 'epoch': 0.15}\n",
      "  6%|‚ñå         | 4000/70000 [32:57<9:02:20,  2.03it/s]{'loss': 5.88, 'learning_rate': 4.714285714285714e-05, 'epoch': 0.15}\n",
      "  6%|‚ñå         | 4100/70000 [33:46<8:52:04,  2.06it/s]{'loss': 5.88, 'learning_rate': 4.707142857142857e-05, 'epoch': 0.16}\n",
      "{'loss': 5.84, 'learning_rate': 4.7e-05, 'epoch': 0.16}\n",
      "  6%|‚ñå         | 4300/70000 [35:22<8:49:10,  2.07it/s]{'loss': 5.8, 'learning_rate': 4.6928571428571435e-05, 'epoch': 0.16}\n",
      "{'loss': 5.76, 'learning_rate': 4.685714285714286e-05, 'epoch': 0.17}\n",
      "{'loss': 5.84, 'learning_rate': 4.678571428571429e-05, 'epoch': 0.17}\n",
      "  7%|‚ñã         | 4600/70000 [37:48<8:43:35,  2.08it/s]{'loss': 5.76, 'learning_rate': 4.671428571428571e-05, 'epoch': 0.18}\n",
      "{'loss': 5.84, 'learning_rate': 4.6642857142857144e-05, 'epoch': 0.18}\n",
      "{'loss': 5.76, 'learning_rate': 4.6571428571428575e-05, 'epoch': 0.18}\n",
      "  7%|‚ñã         | 4812/70000 [39:30<8:43:47,  2.07it/s]"
     ]
    }
   ],
   "source": [
    "from rm_runner import EC2RemoteRunner\n",
    "# create ec2 remote runner\n",
    "runner = EC2RemoteRunner(\n",
    "  instance_type=\"dl1.24xlarge\",\n",
    "  profile=\"hf-sm\",\n",
    "  region=\"us-east-1\",\n",
    "  container=\"huggingface/optimum-habana:4.21.1-pt1.11.0-synapse1.5.0\"\n",
    "  )\n",
    "\n",
    "# launch my script with gaudi_spawn for distributed training\n",
    "runner.launch(\n",
    "    command=f\"python3 gaudi_spawn.py --use_mpi --world_size=8 run_mlm.py {hyperparameters_string}\",\n",
    "    source_dir=\"scripts\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our `hyperparameters` we defined a `max_steps` property, which limited the pre-training to only `100_000` steps. The `100_000` steps with a global batch size of `256` took us around 12,5 hour to pre-train our model. \n",
    "\n",
    "BERT was originial pre-trained on [1 Million Steps](https://arxiv.org/pdf/1810.04805.pdf) with a global batch size of `256`: \n",
    "> We train with batch size of 256 sequences (256 sequences * 512 tokens = 128,000 tokens/batch) for 1,000,000 steps, which is approximately 40 epochs over the 3.3 billion word corpus. \n",
    "\n",
    "Meaning if we want to do a full pre-training the training would take around 125h hours (12,5 hour * 10) and would cost us around ~$1,650 on a Habana Gaudi on AWS, which is extermely cheap. "
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "5fcf248a74081676ead7e77f54b2c239ba2921b952f7cbcdbbe5427323165924"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('hf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
