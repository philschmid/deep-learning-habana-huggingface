{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning BERT with Hugging Face Transformers and Optimum running on the Habana Gaudi Instances\n",
    "\n",
    "In this blog, you will learn how to fine-tune BERT for multi-class text-classification using a Habana Gaudi instance on AWS to save 40% cost and be 2x faster then comparable GPUs.\n",
    "We will use the Hugging Faces Transformers, Optimum and Datasets library to fine-tune a pre-trained transformer for multi-class text classification. In particular, we will fine-tune BERT-Large using the Banking77 dataset. Before we get started, we need to set up the deep learning environment. \n",
    "\n",
    "You will learn how to:\n",
    "\n",
    "1. [Setup Habana Gaudi instance](#1-setup-habana-gaudi-instance)\n",
    "2. [Load and process the dataset](#2-load-and-process-the-dataset)\n",
    "3. [Create a `GaudiTrainer` and an run single HPU fine-tuning](#3-create-a-gauditrainer-and-an-run-single-hpu-fine-tuning)\n",
    "4. [Run distributed data parallel training with `GaudiTrainer`](#4-run-distributed-data-parallel-training-with-gauditrainer)\n",
    "5. [Cost performance benefits of Habana Gaudi1 on AWS](#5-cost-performance-benefits-of-habana-gaudi1-on-aws)\n",
    "\n",
    "\n",
    "### Requirements\n",
    "\n",
    "Before we can start make sure you have met the following requirements\n",
    "\n",
    "* AWS Account with quota for [DL1 instance type](https://aws.amazon.com/ec2/instance-types/dl1/)\n",
    "* [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) installed\n",
    "* AWS IAM user [configured in CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) with permission to create and manage ec2 instances\n",
    "\n",
    "### Helpful Resources\n",
    "\n",
    "* [Optimum Habana Documentation](https://github.com/huggingface/optimum-habana)\n",
    "\n",
    "## 1. Setup Habana Gaudi instance\n",
    "\n",
    "In this example are we going to use Habana Gaudi on AWS using the DL1 instance. We already have created a blog post in the past on how to [Setup Deep Learning environment for Hugging Face Transformers with Habana Gaudi on AWS](https://www.philschmid.de/getting-started-habana-gaudi). If you haven't have read this blog post, please read it first and go through the steps on how to setup the environment. \n",
    "Or if you feel comfortable you can use the `start_instance.sh` in the root of the repository to create your DL1 instance and the continue at step  [4. Use Jupyter Notebook/Lab via ssh](https://www.philschmid.de/getting-started-habana-gaudi#4-use-jupyter-notebooklab-via-ssh) in the Setup blog post.\n",
    "\n",
    "1. run habana docker container an mount current directory\n",
    "```bash\n",
    "docker run -ti --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --net=host --ipc=host -v $(pwd):/home/ubuntu/dev --workdir=/home/ubuntu/dev vault.habana.ai/gaudi-docker/1.4.1/ubuntu20.04/habanalabs/pytorch-installer-1.10.2:1.4.1-11\n",
    "```\n",
    "2. install juptyer\n",
    "```bash\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "3. clone repository\n",
    "```bash\n",
    "git clone https://github.com/philschmid/deep-learning-habana-huggingface.git\n",
    "cd fine-tuning\n",
    "```\n",
    "\n",
    "4. run jupyter notebook\n",
    "```bash\n",
    "jupyter notebook --allow-root\n",
    "#         http://localhost:8888/?token=f8d00db29a6adc03023413b7f234d110fe0d24972d7ae65e\n",
    "```\n",
    "4. continue here\n",
    "\n",
    "_**NOTE**: The following steps assume that the code/cells are running on a gaudi instance with access to HPUs_\n",
    "\n",
    "As first lets make sure we have access to the HPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device available:True\n",
      "device_count:8\n"
     ]
    }
   ],
   "source": [
    "import habana_frameworks.torch.core as htcore\n",
    "\n",
    "print(f\"device available:{htcore.is_available()}\")\n",
    "print(f\"device_count:{htcore.get_device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next lets install our Hugging Face dependencies and `git-lfs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets tensorboard matplotlib pandas sklearn\n",
    "!pip install git+https://github.com/huggingface/optimum-habana.git # workaround until release of optimum-habana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use git-lfs to upload models and artifacts to the hub.\n",
    "#! sudo apt-get install git-lfs\n",
    "!apt-get install git-lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to finish our setup lets log into the [Hugging Face Hub](https://huggingface.co/models) to push our model artifacts, logs and metrics during training and afterwards to the hub. \n",
    "\n",
    "_To be able to push our model to the Hub, you need to register on the [Hugging Face](https://huggingface.co/join)._\n",
    "\n",
    "We will use the `notebook_login` util from the `huggingface_hub` package to log into our account. You can get your token in the settings at [Access Tokens](https://huggingface.co/settings/tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and process the dataset\n",
    "\n",
    "As Dataset we will use the [AmazonScience/massive](https://huggingface.co/datasets/AmazonScience/massive) a multilingual intent(text)-classification dataset. The dataset contains over 1M utterances across 51 languages with annotations for the Natural Language Understanding tasks of intent prediction and slot annotation.\n",
    "\n",
    "We are going to use the:\n",
    "* English - United States (en-US)\n",
    "* German - Germany (de-DE)\n",
    "* French - France (fr-FR)\n",
    "* Italian - Italy (it-IT)\n",
    "* Portuguese - Portugal (pt-PT)\n",
    "* Spanish - Spain (es-ES)\n",
    "* Dutch - Netherlands (nl-NL)\n",
    "\n",
    "splits. The dataset will have ~80 000 datapoints for training and ~14 000 for evaluation equally split across the different languages.\n",
    "\n",
    "The Model which we will fine-tune is [xlm-roberta-large](https://huggingface.co/xlm-roberta-large) a multilingual RoBERTa model. It was pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"xlm-roberta-large\"\n",
    "gaudi_config_id= \"Habana/roberta-large\" # more here: https://huggingface.co/Habana\n",
    "dataset_id = \"AmazonScience/massive\"\n",
    "dataset_configs=[\"en-US\",\"de-DE\",\"fr-FR\",\"it-IT\",\"pt-PT\",\"es-ES\",\"nl-NL\"]\n",
    "\n",
    "seed=33\n",
    "repository_id = \"habana-xlm-r-large-amazon-massive\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change these configuration to your needs, e.g. the `model_id` to another BERT-like model for a different language, e.g. `BERT-Large`. \n",
    "\n",
    "_**NOTE:** Not all 100+ transformers architectures are currently support by `optimum-habana` you can find a list of supported archtiectures in the [validated models](https://github.com/huggingface/optimum-habana#validated-models) section_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `datasets` library to download and preprocess our dataset. As a frist we will load a 7 different configurations and remove the unnecessary features/columns and the concatenate them into a single dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "\n",
    "# the columns we want to keep in the dataset\n",
    "keep_columns = [\"utt\", \"scenario\"]\n",
    "\n",
    "# process individuell datasets\n",
    "proc_lan_dataset_list=[]\n",
    "for lang in dataset_configs:\n",
    "    # load dataset for language\n",
    "    lang_ds = load_dataset(dataset_id, lang)\n",
    "    # only keep the 'utt' & 'scenario column\n",
    "    lang_ds = lang_ds.remove_columns([col for col in lang_ds[\"train\"].column_names if col not in keep_columns])  \n",
    "    # rename the columns to match transformers schema\n",
    "    lang_ds = lang_ds.rename_column(\"utt\", \"text\")\n",
    "    lang_ds = lang_ds.rename_column(\"scenario\", \"label\")\n",
    "    proc_lan_dataset_list.append(lang_ds)\n",
    "    \n",
    "# concat single splits into one\n",
    "train_dataset = concatenate_datasets([ds[\"train\"] for ds in proc_lan_dataset_list])\n",
    "eval_dataset = concatenate_datasets([ds[\"validation\"] for ds in proc_lan_dataset_list])\n",
    "# create datset dict for easier processing\n",
    "dataset = DatasetDict(dict(train=train_dataset,validation=eval_dataset))\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we prepare the dataset for training. Lets take a quick look at the class distribution of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'label'}>]], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = dataset[\"train\"].to_pandas()\n",
    "\n",
    "df.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model we need to convert our \"Natural Language\" to token IDs. This is done by a 🤗 Transformers Tokenizer which will tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary). If you are not sure what this means check out [chapter 6](https://huggingface.co/course/chapter6/1?fw=tf) of the Hugging Face Course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally we add the `truncation=True` and `padding=max_length` to align the length and truncate texts that are bigger than the maximum size allowed by the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "       examples[\"text\"], padding=\"max_length\", truncation=True\n",
    "    )\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(process, batched=True)\n",
    "tokenized_datasets[\"train\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our `dataset` is processed, we can download the pre-trained model and fine-tune it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a `GaudiTrainer` and an run single HPU fine-tuning\n",
    "\n",
    "Normally you would use the [Trainer](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.Trainer) and [TrainingArguments](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.TrainingArguments) to fine-tune a pytorch-based transformer model. Since we are using the `optimum-habana` library, we can use the [GaudiTrainer]() and [GaudiTrainingArguments]() instead. The `GaudiTrainer` is a wrapper around the [Trainer](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.Trainer) and [TrainingArguments](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.TrainingArguments) that allows you to fine-tune a transformer model on a gaudi instance, with a similar API to the [Trainer](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.Trainer) and [TrainingArguments](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.TrainingArguments) classes. Below you see how easy it is to migrate from the [Trainer](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.Trainer) and [TrainingArguments](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.TrainingArguments) classes to the [GaudiTrainer]() and [GaudiTrainingArguments]() classes.\n",
    "\n",
    "```diff\n",
    "-from transformers import Trainer, TrainingArguments \n",
    "+from optimum.habana import GaudiTrainer, GaudiTrainingArguments\n",
    "\n",
    "# define the training arguments\n",
    "-training_args = GaudiTrainingArguments(\n",
    "+training_args = TrainingArguments(\n",
    "+  use_habana=True,\n",
    "+  use_lazy_mode=True,\n",
    "+  gaudi_config_name=path_to_gaudi_config,\n",
    "  ...\n",
    ")\n",
    "\n",
    "# Initialize our Trainer\n",
    "-trainer = Trainer(\n",
    "+trainer = GaudiTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset\n",
    "    ... # other arguments\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we create our `GaudiTrainer` we first need to define a `compute_metrics` function to evaluate our model on the test set. This function will be used during the training process to compute the `accuracy` & `f1` of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "# define metrics and metrics function\n",
    "f1_metric = load_metric(\"f1\")\n",
    "accuracy_metric = load_metric( \"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    acc = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"micro\")\n",
    "    return {\n",
    "        \"accuracy\": acc[\"accuracy\"],\n",
    "        \"f1\": f1[\"f1\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Definition, Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification,DataCollatorWithPadding\n",
    "from optimum.habana import GaudiTrainer, GaudiTrainingArguments\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# create label2id, id2label dicts for nice outputs for the model\n",
    "labels = tokenized_datasets[\"train\"].features[\"label\"].names\n",
    "num_labels = len(labels)\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label\n",
    "\n",
    "\n",
    "# define training args\n",
    "training_args = GaudiTrainingArguments(\n",
    "    output_dir=repository_id,\n",
    "    use_habana=True,\n",
    "    use_lazy_mode=True,\n",
    "    gaudi_config_name=gaudi_config_id,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    ")\n",
    "\n",
    "# define model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels=num_labels, \n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# create Trainer\n",
    "trainer = GaudiTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "# start training on 1x HPU\n",
    "trainer.train()\n",
    "# evaluate model\n",
    "trainer.evaluate(eval_dataset=tokenized_datasets[\"validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run distributed data parallel training with `GaudiTrainer`\n",
    "\n",
    "running the training only on a single HPU-core takes way to long (5h). Luckily with `DL1` instance we have 8 available HPU-cores meaning we can leverage distributed training. \n",
    "To run our training as distributed training we need to create a training script, which can be used with multiprocessing to run on all HPUs. \n",
    "We have created a `scripts/train.py` which contains all the previous steps of the example so far. To executed our distributed training we use the `DistributedRunner` runner from `optimum-habana` alternatively you could check-out the [gaudi_spawn.py](https://github.com/huggingface/optimum-habana/blob/main/examples/gaudi_spawn.py) in the [optimum-habana](https://github.com/huggingface/optimum-habana) repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.habana.distributed import DistributedRunner\n",
    "from optimum.utils import logging\n",
    "\n",
    "world_size=8 # Number of HPUs to use (1 or 8)\n",
    "\n",
    "# define distributed runner\n",
    "distributed_runner = DistributedRunner(\n",
    "    command_list=[\"scripts/train.py --use_habana True\"],\n",
    "    world_size=world_size,\n",
    "    use_mpi=True,\n",
    "    multi_hls=False,\n",
    ")\n",
    "\n",
    "# start job\n",
    "ret_code = distributed_runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cost performance benefits of Habana Gaudi1 on AWS\n",
    "\n",
    "The distributed training on all 8x HPUs took in total 52 minutes. The [dl1.24xlarge](https://aws.amazon.com/ec2/instance-types/dl1/) instance on AWS costs $13.11 per hour leading to only $11,55 for our training. \n",
    "To provide a cost-performance comparison we run the same training on the AWS [p3.8xlarge](https://aws.amazon.com/ec2/instance-types/p3/?nc1=h_ls) instance, which costs roughly the same with $12.24, but only has 4x accelerators (4x NVIDIA V100). The training on the p3.8xlarge instance took in total about 439 minutes and cost $89.72.\n",
    "Meaning the Habana Gaudi instance is **8.4x faster** and **7.7x cheaper** than the price equivalent NVIDIA powered instance. \n",
    "Below is a detailed table of results. Additional both models are available on the Hugging Face Hub at [philschmid/habana-xlm-r-large-amazon-massive](https://huggingface.co/philschmid/habana-xlm-r-large-amazon-massive) and [philschmid/gpu-xlm-roberta-large-amazon-massive](https://huggingface.co/philschmid/gpu-xlm-roberta-large-amazon-massive)\n",
    "\n",
    "\n",
    "| accelerator        | training time (in minutes) | total cost | total batch size | aws instance type                                                    | instance price per hour |\n",
    "|--------------------|----------------------------|------------|------------------|----------------------------------------------------------------------|-------------------------|\n",
    "| Habana Gaudi (HPU) | 52.6                       | $11.55     | 64               | [dl1.24xlarge](https://aws.amazon.com/ec2/instance-types/dl1/)       | $13.11                  |\n",
    "| NVIDIA V100 (GPU)  |     439.8                       |  $89.72          |  4                | [p3.8xlarge](https://aws.amazon.com/ec2/instance-types/p3/?nc1=h_ls) | $12.24                  |\n",
    "\n",
    "\n",
    "_Note: This comparison currently provides a limited view, since the NVIDIA V100 might not be the best GPU available for training such a large transformer model resulting in a 8x smaller batch size. We plan to run a more detailed cost-performance benchmark including more instances, like NVIDIA A100 and more models, e.g. DistilBERT, GPT-2_\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "That's it for this tutorial. Now you know how to fine-tune Hugging Face Transformers on Habana Gaudi using Optimum. You learned how easily you can migrate from a `Trainer` based script to a `GaudiTrainer` based script and how to scale the training to multiple HPUs using the `DistributedRunner`. \n",
    "\n",
    "Additionally, we run a simple cost performance benchmark acheiving **8.4x faster** and **7.7x cheaper** training on Habana Gaudi instance than on the price equivalent NVIDIA powered instance. \n",
    "\n",
    "Now it is time for you to migrate your training scripts!! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
