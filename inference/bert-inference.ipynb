{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT inference using Habana Gaudi and Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device available:True\n",
      "device_count:8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import habana_frameworks.torch as ht\n",
    "import habana_frameworks.torch.core as htcore\n",
    "\n",
    "print(f\"device available:{ht.hpu.is_available()}\")\n",
    "print(f\"device_count:{ht.hpu.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# set device and load model\n",
    "device = torch.device('hpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\").to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define payload\n",
    "payload = \"i like you. I love you.\"\n",
    "\n",
    "# tokenize input\n",
    "enc = tokenizer(payload, return_tensors=\"pt\")\n",
    "\n",
    "# place encodings on hpu\n",
    "enc = {key: value.to(device) for key,value in enc.items()}\n",
    "assert enc[\"input_ids\"].device.type == \"hpu\", \"Inputs not on correct device\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model predicted for input:\n",
      "\"i like you. I love you.\"\n",
      "\"{'label': '5 stars', 'score': 0.8011513948440552}\"\n"
     ]
    }
   ],
   "source": [
    "# run forward pass\n",
    "with torch.no_grad():\n",
    "    output = model(**enc)\n",
    "    # in lazy mode execution, :code:`mark_step()` must be added after model inference\n",
    "    htcore.mark_step()\n",
    "    score = output.logits.softmax(dim=-1)[0]\n",
    "    \n",
    "# created nice output\n",
    "pred = {\"label\": model.config.id2label[score.argmax().cpu().item()], \"score\": score.max().cpu().item()}    \n",
    "\n",
    "print(f'model predicted for input:\\n\"{payload}\"\\n\"{pred}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hpu_txt_pipeline(inputs,model,tokenizer):\n",
    "    # tokenize input\n",
    "    enc = tokenizer(inputs, return_tensors=\"pt\")\n",
    "    # place encodings on hpu\n",
    "    enc = {key: value.to(device) for key,value in enc.items()}\n",
    "    # run forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(**enc)\n",
    "        # in lazy mode execution, :code:`mark_step()` must be added after model inference\n",
    "        htcore.mark_step()\n",
    "        score = output.logits.softmax(dim=-1)[0]\n",
    "    return {\"label\": model.config.id2label[score.argmax().cpu().item()], \"score\": score.max().cpu().item()}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.2 ms ± 17.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "{'label': '5 stars', 'score': 0.47468531131744385}\n",
      "12.1 ms ± 14.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "{'label': '3 stars', 'score': 0.7794715762138367}\n",
      "12.2 ms ± 16.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "{'label': '1 star', 'score': 0.809077262878418}\n"
     ]
    }
   ],
   "source": [
    "%timeit positive_sentiment = hpu_txt_pipeline(\"I like you\",model,tokenizer)\n",
    "print(positive_sentiment)\n",
    "\n",
    "%timeit neutral_sentiment = hpu_txt_pipeline(\"The movie was okay\",model,tokenizer)\n",
    "print(neutral_sentiment)\n",
    "\n",
    "%timeit negative_sentiment = hpu_txt_pipeline(\"The fish was horrible, i got sick\",model,tokenizer)\n",
    "print(negative_sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payload sequence length is: 128\n",
      "model: P95 latency (ms) - 9.70917100000861; Average latency (ms) - 9.68 +\\- 0.13;\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "import numpy as np\n",
    "\n",
    "payload=\"Hello my name is Philipp. I am getting in touch with you because i didn't get a response from you. What do I need to do to get my new card which I have requested 2 weeks ago? Please help me and answer this email in the next 7 days. Best regards and have a nice weekend \"*2\n",
    "payload=payload\n",
    "print(f'Payload sequence length is: {len(tokenizer(payload)[\"input_ids\"])}')\n",
    "\n",
    "\n",
    "def prep_payload(payload):\n",
    "    enc = tokenizer(payload, return_tensors=\"pt\")\n",
    "    return {key: value.to(device) for key,value in enc.items()}\n",
    "\n",
    "\n",
    "def measure_latency(model,payload):\n",
    "    enc = prep_payload(payload)\n",
    "    latencies = []\n",
    "    # warm up\n",
    "    for _ in range(10):\n",
    "        with torch.no_grad():\n",
    "            _ = model(**enc)\n",
    "            htcore.mark_step()\n",
    "    # Timed run\n",
    "    for _ in range(300):\n",
    "        start_time = perf_counter()\n",
    "        with torch.no_grad():\n",
    "            _ = model(**enc)\n",
    "            htcore.mark_step()\n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "    # Compute run statistics\n",
    "    time_avg_ms = 1000 * np.mean(latencies)\n",
    "    time_std_ms = 1000 * np.std(latencies)\n",
    "    time_p95_ms = 1000 * np.percentile(latencies,95)\n",
    "    return f\"P95 latency (ms) - {time_p95_ms}; Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f};\", time_p95_ms\n",
    "\n",
    "hpu_run=measure_latency(model,payload)\n",
    "print(f\"model: {hpu_run[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "```bash\n",
    "Payload sequence length is: 128\n",
    "model: P95 latency (ms) - 9.79173000002902; Average latency (ms) - 9.76 +\\- 0.02;\n",
    "Payload sequence length is: 506\n",
    "model: P95 latency (ms) - 9.774564700057908; Average latency (ms) - 9.74 +\\- 0.11;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "5fcf248a74081676ead7e77f54b2c239ba2921b952f7cbcdbbe5427323165924"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
