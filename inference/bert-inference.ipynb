{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT inference using Habana Gaudi and Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device available:True\n",
      "device_count:8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import habana_frameworks.torch as ht\n",
    "import habana_frameworks.torch.core as htcore\n",
    "\n",
    "print(f\"device available:{ht.hpu.is_available()}\")\n",
    "print(f\"device_count:{ht.hpu.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015911579132080078,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 20,
       "postfix": null,
       "prefix": "Downloading tokenizer_config.json",
       "rate": null,
       "total": 401,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db8d06cb73654d249339f97508c26d0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/401 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011885881423950195,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 20,
       "postfix": null,
       "prefix": "Downloading vocab.txt",
       "rate": null,
       "total": 231508,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a32d5a16e8d48f38a12a25c9cc09af4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012095212936401367,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 20,
       "postfix": null,
       "prefix": "Downloading special_tokens_map.json",
       "rate": null,
       "total": 125,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "053f27caf33d4060b14ba16038190540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01217794418334961,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 20,
       "postfix": null,
       "prefix": "Downloading config.json",
       "rate": null,
       "total": 883,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f3779eb3bb4b039d018bcf3751cfcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/883 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012791633605957031,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 20,
       "postfix": null,
       "prefix": "Downloading pytorch_model.bin",
       "rate": null,
       "total": 1340710317,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe2d964ea5a44230b625c5d69b76a494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# bert-base\n",
    "# model_id=\"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "# bert-large\n",
    "model_id=\"Farshid/bert-large-uncased-financial-phrasebank-allagree2\"\n",
    "\n",
    "# set device and load model\n",
    "device = torch.device('hpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define payload\n",
    "payload = \"i like you. I love you.\"\n",
    "\n",
    "# tokenize input\n",
    "enc = tokenizer(payload, return_tensors=\"pt\")\n",
    "\n",
    "# place encodings on hpu\n",
    "enc = {key: value.to(device) for key,value in enc.items()}\n",
    "assert enc[\"input_ids\"].device.type == \"hpu\", \"Inputs not on correct device\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model predicted for input:\n",
      "\"i like you. I love you.\"\n",
      "\"{'label': 'LABEL_1', 'score': 0.9986135363578796}\"\n"
     ]
    }
   ],
   "source": [
    "# run forward pass\n",
    "with torch.no_grad():\n",
    "    output = model(**enc)\n",
    "    # in lazy mode execution, :code:`mark_step()` must be added after model inference\n",
    "    htcore.mark_step()\n",
    "    score = output.logits.softmax(dim=-1)[0]\n",
    "    \n",
    "# created nice output\n",
    "pred = {\"label\": model.config.id2label[score.argmax().cpu().item()], \"score\": score.max().cpu().item()}    \n",
    "\n",
    "print(f'model predicted for input:\\n\"{payload}\"\\n\"{pred}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hpu_txt_pipeline(inputs,model,tokenizer):\n",
    "    # tokenize input\n",
    "    enc = tokenizer(inputs, return_tensors=\"pt\")\n",
    "    # place encodings on hpu\n",
    "    enc = {key: value.to(device) for key,value in enc.items()}\n",
    "    # run forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(**enc)\n",
    "        # in lazy mode execution, :code:`mark_step()` must be added after model inference\n",
    "        htcore.mark_step()\n",
    "        score = output.logits.softmax(dim=-1)[0]\n",
    "    return {\"label\": model.config.id2label[score.argmax().cpu().item()], \"score\": score.max().cpu().item()}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 ms ± 38.2 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "{'label': 'LABEL_1', 'score': 0.9652733206748962}\n",
      "23.4 ms ± 1e+03 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "{'label': 'LABEL_1', 'score': 0.9984086155891418}\n",
      "23.7 ms ± 958 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "{'label': 'LABEL_0', 'score': 0.9787769317626953}\n"
     ]
    }
   ],
   "source": [
    "%timeit hpu_txt_pipeline(\"I like you\",model,tokenizer)\n",
    "positive_sentiment = hpu_txt_pipeline(\"I like you\",model,tokenizer)\n",
    "print(positive_sentiment)\n",
    "\n",
    "%timeit hpu_txt_pipeline(\"The movie was okay\",model,tokenizer)\n",
    "neutral_sentiment = hpu_txt_pipeline(\"The movie was okay\",model,tokenizer)\n",
    "print(neutral_sentiment)\n",
    "\n",
    "%timeit hpu_txt_pipeline(\"The fish was horrible, i got sick\",model,tokenizer)\n",
    "negative_sentiment = hpu_txt_pipeline(\"The fish was horrible, i got sick\",model,tokenizer)\n",
    "print(negative_sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payload sequence length is: 506\n",
      "model: P95 latency (ms) - 18.704093350032736; Average latency (ms) - 18.73 +\\- 1.23;\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "import numpy as np\n",
    "\n",
    "payload=\"Hello my name is Philipp. I am getting in touch with you because i didn't get a response from you. What do I need to do to get my new card which I have requested 2 weeks ago? Please help me and answer this email in the next 7 days. Best regards and have a nice weekend \"*2\n",
    "payload=payload*4\n",
    "print(f'Payload sequence length is: {len(tokenizer(payload)[\"input_ids\"])}')\n",
    "\n",
    "\n",
    "def prep_payload(payload):\n",
    "    enc = tokenizer(payload, return_tensors=\"pt\")\n",
    "    return {key: value.to(device) for key,value in enc.items()}\n",
    "\n",
    "\n",
    "def measure_latency(model,payload):\n",
    "    enc = prep_payload(payload)\n",
    "    latencies = []\n",
    "    # warm up\n",
    "    for _ in range(10):\n",
    "        with torch.no_grad():\n",
    "            _ = model(**enc)\n",
    "            htcore.mark_step()\n",
    "    # Timed run\n",
    "    for _ in range(300):\n",
    "        start_time = perf_counter()\n",
    "        with torch.no_grad():\n",
    "            _ = model(**enc)\n",
    "            htcore.mark_step()\n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "    # Compute run statistics\n",
    "    time_avg_ms = 1000 * np.mean(latencies)\n",
    "    time_std_ms = 1000 * np.std(latencies)\n",
    "    time_p95_ms = 1000 * np.percentile(latencies,95)\n",
    "    return f\"P95 latency (ms) - {time_p95_ms}; Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f};\", time_p95_ms\n",
    "\n",
    "hpu_run=measure_latency(model,payload)\n",
    "print(f\"model: {hpu_run[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "**BERT-base**\n",
    "\n",
    "```bash\n",
    "Payload sequence length is: 128\n",
    "model: P95 latency (ms) - 9.79173000002902; Average latency (ms) - 9.76 +\\- 0.02;\n",
    "Payload sequence length is: 506\n",
    "model: P95 latency (ms) - 9.774564700057908; Average latency (ms) - 9.74 +\\- 0.11;\n",
    "```\n",
    "\n",
    "**BERT-large**\n",
    "\n",
    "```bash\n",
    "Payload sequence length is: 128\n",
    "model: P95 latency (ms) - 18.61147640003651; Average latency (ms) - 18.53 +\\- 0.07;\n",
    "Payload sequence length is: 506\n",
    "model: P95 latency (ms) - 18.704093350032736; Average latency (ms) - 18.73 +\\- 1.23;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "5fcf248a74081676ead7e77f54b2c239ba2921b952f7cbcdbbe5427323165924"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
